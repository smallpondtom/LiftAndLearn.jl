var documenterSearchIndex = {"docs":
[{"location":"manual/Lift/#Lifting","page":"Lift","title":"Lifting","text":"","category":"section"},{"location":"manual/Lift/","page":"Lift","title":"Lift","text":"This page gives an explanation for the mathematical concept of lifting and provides example code for its implementation.","category":"page"},{"location":"manual/Lift/#Lift-Map-Structure","page":"Lift","title":"Lift Map Structure","text":"","category":"section"},{"location":"manual/Lift/","page":"Lift","title":"Lift","text":"lifting","category":"page"},{"location":"manual/Lift/#LiftAndLearn.lifting","page":"Lift","title":"LiftAndLearn.lifting","text":"struct lifting\n\nLifting map structure.\n\nFields\n\nN: number of variables of the original nonlinear dynamics\nNl: number of variables of the lifted system\nlift_funcs: array of lifting transformation functions \nmap: function to map the data to the new mapped states including original states\nmapNL: function to map the data to only the additional lifted states \n\n\n\n\n\n","category":"type"},{"location":"manual/Lift/","page":"Lift","title":"Lift","text":"tip: Tip\nFor a simple pendulum we have beginbmatrix\ndotx_1 \ndotx_2 \nendbmatrix = beginbmatrix\nx_2 \n-fracgl sin(x_1)\nendbmatrixThe lifted system becomes beginbmatrix\ndotx_1 \ndotx_2 \ndotx_3 \ndotx_4\nendbmatrix = beginbmatrix\nx_2 \n-fracgl x_3 \nx_2 x_4 \n-x_2 x_3\nendbmatrixwhen x_3 = sin(x_1) and x_4 = cos(x_1). Which if coded, would look like this:lifter = LnL.lifting(2, 4, [x -> sin.(x[1]), x -> cos.(x[1])])","category":"page"},{"location":"manual/Lift/#Construct-Lifted-POD-Basis-from-Data","page":"Lift","title":"Construct Lifted POD Basis from Data","text":"","category":"section"},{"location":"manual/Lift/","page":"Lift","title":"Lift","text":"lifted_basis","category":"page"},{"location":"manual/Lift/#LiftAndLearn.lifted_basis","page":"Lift","title":"LiftAndLearn.lifted_basis","text":"lifted_basis(W, Nl, gp, ro) → Vr\n\nCreate the block-diagonal POD basis for the new lifted system data\n\nArguments\n\nw: lifted data matrix\nNl: number of variables of the lifted state dynamics\ngp: number of grid points for each variable\nro: vector of the reduced orders for each basis\n\nReturn\n\nVr: block diagonal POD basis\n\n\n\n\n\n","category":"function"},{"location":"manual/Lift/","page":"Lift","title":"Lift","text":"An example implementation would be:","category":"page"},{"location":"manual/Lift/","page":"Lift","title":"Lift","text":"using LiftAndLearn\nusing Random\nLnL = LiftAndLearn\nW = round.(rand(30,100), digits=4)\nLnL.lifted_basis(W, 3, 10, [2,3,4])","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"CurrentModule = LiftAndLearn ","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Pages = [\"api.md\"]","category":"page"},{"location":"api/#API","page":"API Reference","title":"API","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"All APIs of LiftAndLearn listed in a unstructured manner.","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Modules = [LiftAndLearn]\nOrder = [:module, :function, :macro]","category":"page"},{"location":"api/#LiftAndLearn.LiftAndLearn","page":"API Reference","title":"LiftAndLearn.LiftAndLearn","text":"LiftAndLearn package main module\n\n\n\n\n\n","category":"module"},{"location":"api/#LiftAndLearn.choose_ro-Tuple{Vector}","page":"API Reference","title":"LiftAndLearn.choose_ro","text":"choose_ro(Σ::Vector; en_low=-15) → r_all, en\n\nChoose reduced order (ro) that preserves an acceptable energy.\n\nArguments\n\nΣ::Vector: Singular value vector from the SVD of some Hankel Matrix\nen_low: minimum size for energy preservation\n\nReturns\n\nr_all: vector of reduced orders\nen: vector of energy values\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.compute_all_errors-NTuple{7, Any}","page":"API Reference","title":"LiftAndLearn.compute_all_errors","text":"compute_all_errors(Xf, Yf, Xint, Yint, Xinf, Yinf, Vr) → PE, ISE, IOE, OSE, OOE\n\nCompute all projection, state, and output errors\n\nArguments\n\nXf: reference state data\nYf: reference output data\nXint: intrusive model state data\nYint: intrusive model output data\nXinf: inferred model state data\nXint: inferrred model output data\nVr: POD basis\n\nReturn\n\nPE: projection error\nISE: intrusive state error\nIOE: intrusive output error\nOSE: operator inference state error\nOOE: operator inference output error\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.delta-Tuple{Any, Any}","page":"API Reference","title":"LiftAndLearn.delta","text":"delta(v::Int, w::Int) → Float64\n\nAnother auxiliary function for the F matrix\n\nArguments\n\nv: first index\nw: second index\n\nReturns\n\ncoefficient of 1.0 or 0.5\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.ep_constraint_residual","page":"API Reference","title":"LiftAndLearn.ep_constraint_residual","text":"ep_constraint_residual(X, r)\nep_constraint_residual(X, r, redundant; with_moment)\n\n\nCompute the constraint residual which is the residual of the energy-preserving constraint \n\nsum left hath_ijk + hath_jik + hath_kji right quad 1 leq ijk leq r\n\nArguments\n\nX::AbstractArray: the matrix to compute the constraint residual\nr::Real: the dimension of the system\nredundant::Bool: redundant or nonredundant operator\nwith_moment::Bool: whether to compute the moment of the constraint residual\n\nReturns\n\nϵX: the constraint residual\nmmt: the moment which is the sum of the constraint residual without absolute value\n\n\n\n\n\n","category":"function"},{"location":"api/#LiftAndLearn.ep_constraint_violation","page":"API Reference","title":"LiftAndLearn.ep_constraint_violation","text":"ep_constraint_violation(Data, X)\nep_constraint_violation(Data, X, redundant)\n\n\nCompute the constraint violation which is the violation of the energy-preserving constraint\n\nsum langle mathbfx mathbfH(mathbfxotimesmathbfx)rangle quad forall mathbfx in mathcalD\n\nArguments\n\nData::AbstractArray: the data\nX::AbstractArray: the matrix to compute the constraint violation\nredundant::Bool: redundant or nonredundant operator\n\nReturns\n\nviol: the constraint violation\n\n\n\n\n\n","category":"function"},{"location":"api/#LiftAndLearn.ephec_opinf-Tuple{Matrix, Union{Matrix, LinearAlgebra.Transpose}, AbstractArray, AbstractArray, LiftAndLearn.AbstractOption, Operators}","page":"API Reference","title":"LiftAndLearn.ephec_opinf","text":"ephec_opinf(D, Rt, dims, operators_symbols, options, IG)\n\n\nEnergy preserved (Hard Equality Constraint) operator inference optimization (EPHEC)\n\nArguments\n\nD: data matrix\nRt: transpose of the derivative matrix (or residual matrix)\ndims: dimensions of the operators\noperators_symbols: symbols of the operators\noptions: options for the operator inference set by the user\nIG: Initial Guesses\n\nReturns\n\nInferred operators\n\nNote\n\nThis is currently implemented for linear + quadratic operators only\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.epopinf-Tuple{AbstractArray, AbstractArray, LiftAndLearn.AbstractOption}","page":"API Reference","title":"LiftAndLearn.epopinf","text":"epopinf(X, Vn, options; U, Xdot, IG)\n\n\nEnergy-preserving Operator Inference (EPOpInf) optimization problem.\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.epp_opinf-Tuple{Matrix, Union{Matrix, LinearAlgebra.Transpose}, AbstractArray, AbstractArray, LiftAndLearn.AbstractOption, Operators}","page":"API Reference","title":"LiftAndLearn.epp_opinf","text":"epp_opinf(D, Rt, dims, operators_symbols, options, IG)\n\n\nEnergy preserving penalty operator inference optimization (EPP)\n\nArguments\n\nD: data matrix\nRt: transpose of the derivative matrix (or residual matrix)\ndims: dimensions of the operators\noperators_symbols: symbols of the operators\noptions: options for the operator inference set by the user\nIG: Initial Guesses\n\nReturns\n\nInferred operators\n\nNote\n\nThis is currently implemented for linear + quadratic operators only\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.epsic_opinf-Tuple{Matrix, Union{Matrix, LinearAlgebra.Transpose}, AbstractArray, AbstractArray, LiftAndLearn.AbstractOption, Operators}","page":"API Reference","title":"LiftAndLearn.epsic_opinf","text":"epsic_opinf(D, Rt, dims, operators_symbols, options, IG)\n\n\nEnergy preserved (Soft Inequality Constraint) operator inference optimization (EPSIC)\n\nArguments\n\nD: data matrix\nRt: transpose of the derivative matrix (or residual matrix)\ndims: dimensions of the operators\noperators_symbols: symbols of the operators\noptions: options for the operator inference set by the user\nIG: Initial Guesses\n\nReturns\n\nInferred operators\n\nNote\n\nThis is currently implemented for linear + quadratic operators only\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.fat2tall-Tuple{AbstractArray}","page":"API Reference","title":"LiftAndLearn.fat2tall","text":"fat2tall(A::AbstractArray)\n\nConvert a fat matrix to a tall matrix by taking the transpose if the number  of rows is less than the number of columns.\n\nArguments\n\nA::AbstractArray: input matrix\n\nReturns\n\nA::AbstractArray: output matrix\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.fidx-Tuple{Any, Any, Any}","page":"API Reference","title":"LiftAndLearn.fidx","text":"fidx(n::Int, j::Int, k::Int) → Int\n\nAuxiliary function for the F matrix indexing.\n\nArguments\n\nn: row dimension of the F matrix\nj: row index \nk: col index\n\nReturns\n\nindex corresponding to the F matrix\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.get_data_matrix-Tuple{AbstractArray, AbstractArray, AbstractArray, LiftAndLearn.AbstractOption}","page":"API Reference","title":"LiftAndLearn.get_data_matrix","text":"get_data_matrix(Xhat, Xhat_t, Ut, options; verbose)\n\n\nGet the data matrix for the regression problem\n\nArguments\n\nXhat::AbstractArray: projected data matrix\nXhat_t::AbstractArray: projected data matrix (transposed)\nUt::AbstractArray: input data matrix (transposed)\noptions::AbstractOption: options for the operator inference set by the user\nverbose::Bool=false: verbose mode returning the dimension breakdown and operator symbols\n\nReturns\n\nD: data matrix for the regression problem\ndims: dimension breakdown of the data matrix\noperator_symbols: operator symbols corresponding to dims for the regression problem\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.get_data_matrix-Tuple{AbstractArray, AbstractArray, LiftAndLearn.AbstractOption}","page":"API Reference","title":"LiftAndLearn.get_data_matrix","text":"get_data_matrix(Xhat, Ut, options)\n\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.isenergypreserving","page":"API Reference","title":"LiftAndLearn.isenergypreserving","text":"isenergypreserving(X)\nisenergypreserving(X, redundant; tol)\n\n\nCheck if the matrix is energy-preserving.\n\nArguments\n\nX::AbstractArray: the matrix to check if it is energy-preserving\nredundant::Bool: redundant or nonredundant operator\ntol::Real: the tolerance\n\nReturns\n\nBool: whether the matrix is energy-preserving\n\n\n\n\n\n","category":"function"},{"location":"api/#LiftAndLearn.leastsquares_solve-Tuple{AbstractArray, AbstractArray, AbstractArray, AbstractArray, AbstractArray, AbstractArray, LiftAndLearn.AbstractOption}","page":"API Reference","title":"LiftAndLearn.leastsquares_solve","text":"leastsquares_solve(D::AbstractArray, Rt::AbstractArray, Y::AbstractArray, Xhat_t::AbstractArray, \n         dims::AbstractArray, operator_symbols::AbstractArray, options::AbstractOption)\n\nSolve the standard Operator Inference with/without regularization\n\nArguments\n\nD::AbstractArray: data matrix\nRt::AbstractArray: derivative data matrix (tall)\nYt::AbstractArray: output data matrix (tall)\nXhat_t::AbstractArray: projected data matrix (tall)\ndims::AbstractArray: dimensions of the operators\noperator_symbols::AbstractArray: symbols of the operators\noptions::AbstractOption: options for the operator inference set by the user\n\nReturns\n\noperators::Operators: All learned operators\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.lifted_basis-Tuple{Matrix, Real, Integer, Vector}","page":"API Reference","title":"LiftAndLearn.lifted_basis","text":"lifted_basis(W, Nl, gp, ro) → Vr\n\nCreate the block-diagonal POD basis for the new lifted system data\n\nArguments\n\nw: lifted data matrix\nNl: number of variables of the lifted state dynamics\ngp: number of grid points for each variable\nro: vector of the reduced orders for each basis\n\nReturn\n\nVr: block diagonal POD basis\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.opinf-Tuple{AbstractArray, AbstractArray, LiftAndLearn.AbstractOption}","page":"API Reference","title":"LiftAndLearn.opinf","text":"opinf(X::AbstractArray, Vn::AbstractArray, options::AbstractOption; \n    U::AbstractArray=zeros(1,1), Y::AbstractArray=zeros(1,1),\n    Xdot::AbstractArray=[]) → op::Operators\n\nInfer the operators with derivative data given. NOTE: Make sure the data is  constructed such that the row is the state vector and the column is the time.\n\nArguments\n\nX::AbstractArray: state data matrix\nVn::AbstractArray: POD basis\noptions::AbstractOption: options for the operator inference defined by the user\nU::AbstractArray: input data matrix\nY::AbstractArray: output data matix\nXdot::AbstractArray: derivative data matrix\n\nReturns\n\nop::Operators: inferred operators\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.opinf-Tuple{AbstractArray, AbstractArray, Operators, LiftAndLearn.AbstractOption}","page":"API Reference","title":"LiftAndLearn.opinf","text":"opinf(X::AbstractArray, Vn::AbstractArray, full_op::Operators, options::AbstractOption;\n    U::AbstractArray=zeros(1,1), Y::AbstractArray=zeros(1,1)) → op::Operators\n\nInfer the operators with reprojection method (dispatch). NOTE: Make sure the data is constructed such that the row is the state vector and the column is the time.\n\nArguments\n\nX::AbstractArray: state data matrix\nVn::AbstractArray: POD basis\nfull_op::Operators: full order model operators\noptions::AbstractOption: options for the operator inference defined by the user\nU::AbstractArray: input data matrix\nY::AbstractArray: output data matix\nreturn_derivative::Bool=false: return the derivative matrix (or residual matrix)\n\nReturns\n\nop::Operators: inferred operators\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.opinf-Tuple{AbstractArray, AbstractArray, lifting, Operators, LiftAndLearn.AbstractOption}","page":"API Reference","title":"LiftAndLearn.opinf","text":"opinf(W::AbstractArray, Vn::AbstractArray, lm::lifting, full_op::Operators,\n        options::AbstractOption; U::AbstractArray=zeros(1,1), \n        Y::AbstractArray=zeros(1,1), IG::Operators=Operators()) → op::Operators\n\nInfer the operators for Lift And Learn for reprojected data (dispatch). NOTE: make sure that the data is constructed such that the row dimension is the state dimension and the column dimension is the time dimension. \n\nArguments\n\nW::AbstractArray: state data matrix\nVn::AbstractArray: POD basis\nlm::lifting: struct of the lift map\nfull_op::Operators: full order model operators\noptions::AbstractOption: options for the operator inference defined by the user\nU::AbstractArray: input data matrix\nY::AbstractArray: output data matix\n\nReturns\n\nop::Operators: inferred operators\n\nNote\n\nYou can opt to use the unlifted version of the opinf function instead of this dispatch if it is not necessary to reproject the data.\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.pod-Tuple{Operators, AbstractArray, SystemStructure}","page":"API Reference","title":"LiftAndLearn.pod","text":"pod(op, Vr, sys_struct; nonredundant_operators)\n\n\nPerform intrusive model reduction using Proper Orthogonal Decomposition (POD). This implementation is liimted to\n\nstate: up to 4th order\ninput: only B matrix\noutput: only C and D matrices\nstate-input-coupling: bilinear \nconstant term: K matrix\n\nArguments\n\nop: operators of the target system \nVr: POD basis\noptions: options for the operator inference\n\nReturn\n\nop_new: new operator projected onto the basis\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.proj_error-Tuple{Any, Any}","page":"API Reference","title":"LiftAndLearn.proj_error","text":"proj_error(Xf, Vr) → PE\n\nCompute the projection error\n\nArguments\n\nXf: reference state data\nVr: POD basis\n\nReturn\n\nPE: projection error\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.rel_output_error-Tuple{Any, Any}","page":"API Reference","title":"LiftAndLearn.rel_output_error","text":"rel_output_error(Yf, Y) → OE\n\nCompute relative output error\n\nArguments\n\nYf: reference output data\nY: testing output data\n\nReturn\n\nOE: output error\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.rel_state_error-Tuple{Any, Any, Any}","page":"API Reference","title":"LiftAndLearn.rel_state_error","text":"rel_state_error(Xf, X, Vr) → SE\n\nCompute the relative state error\n\nArguments\n\nXf: reference state data\nX: testing state data\nVr: POD basis\n\nReturn\n\nSE: state error\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.reproject-Tuple{AbstractArray, AbstractArray, AbstractArray, Operators, LiftAndLearn.AbstractOption}","page":"API Reference","title":"LiftAndLearn.reproject","text":"reproject(Xhat::AbstractArray, V::AbstractArray, Ut::AbstractArray,\n    op::Operators, options::AbstractOption) → Rhat::AbstractArray\n\nReprojecting the data to minimize the error affected by the missing orders of the POD basis\n\nArguments\n\nXhat::AbstractArray: state data matrix projected onto the basis\nV::AbstractArray: POD basis\nUt::AbstractArray: input data matrix (tall)\nop::Operators: full order model operators\noptions::AbstractOption: options for the operator inference defined by the user\n\nReturn\n\nRhat::AbstractArray: R matrix (transposed) for the regression problem\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.reproject-Tuple{AbstractArray, AbstractArray, AbstractArray, lifting, Operators, LiftAndLearn.AbstractOption}","page":"API Reference","title":"LiftAndLearn.reproject","text":"reproject(Xhat::Matrix, V::Union{VecOrMat,BlockDiagonal}, U::VecOrMat,\n    lm::lifting, op::Operators, options::AbstractOption) → Rhat::Matrix\n\nReprojecting the lifted data\n\nArguments\n\nXhat::AbstractArray: state data matrix projected onto the basis\nV::AbstractArray: POD basis\nUt::AbstractArray: input data matrix (tall)\nlm::lifting: struct of the lift map\nop::Operators: full order model operators\noptions::AbstractOption: options for the operator inference defined by the user\n\nReturns\n\nRhat::Matrix: R matrix (transposed) for the regression problem\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.tall2fat-Tuple{AbstractArray}","page":"API Reference","title":"LiftAndLearn.tall2fat","text":"tall2fat(A::AbstractArray)\n\nConvert a tall matrix to a fat matrix by taking the transpose if the number of rows is less than the number of columns.\n\nArguments\n\nA::AbstractArray: input matrix\n\nReturns\n\nA::AbstractArray: output matrix\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.tikhonov-Tuple{AbstractArray, AbstractArray, AbstractMatrix, Real}","page":"API Reference","title":"LiftAndLearn.tikhonov","text":"tikhonov(b::AbstractArray, A::AbstractArray, Γ::AbstractMatrix, tol::Real;\n    flag::Bool=false)\n\nTikhonov regression\n\nArguments\n\nb::AbstractArray: right hand side of the regression problem\nA::AbstractArray: left hand side of the regression problem\nΓ::AbstractMatrix: Tikhonov matrix\ntol::Real: tolerance for the singular values\nflag::Bool: flag for the tolerance\n\nReturns\n\nregression solution\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.tikhonovMatrix!-Tuple{AbstractArray, AbstractArray, AbstractArray, TikhonovParameter}","page":"API Reference","title":"LiftAndLearn.tikhonovMatrix!","text":"tikhonovMatrix!(Γ::AbstractArray, dims::Dict, options::AbstractOption)\n\nConstruct the Tikhonov matrix\n\nArguments\n\nΓ::AbstractArray: Tikhonov matrix (pass by reference)\noptions::AbstractOption: options for the operator inference set by the user\n\nReturns\n\nΓ: Tikhonov matrix (pass by reference)\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.time_derivative_approx-Tuple{VecOrMat, LiftAndLearn.AbstractOption}","page":"API Reference","title":"LiftAndLearn.time_derivative_approx","text":"time_derivative_approx(X, options)\n\n\nApproximating the derivative values of the data with different integration schemes\n\nArguments\n\nX::VecOrMat: data matrix\noptions::AbstractOption: operator inference options\n\nReturns\n\ndXdt: derivative data\nidx: index for the specific integration scheme (important for later use)\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.unpack_operators!-Tuple{Operators, AbstractArray, AbstractArray, AbstractArray, AbstractArray, AbstractArray, LiftAndLearn.AbstractOption}","page":"API Reference","title":"LiftAndLearn.unpack_operators!","text":"unpack_operators!(\n    operators,\n    O,\n    Yt,\n    Xhat_t,\n    dims,\n    operator_symbols,\n    options\n)\n\n\nUnpack the operators from the operator matrix O including the output.\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.unpack_operators!-Tuple{Operators, AbstractArray, AbstractArray, AbstractArray}","page":"API Reference","title":"LiftAndLearn.unpack_operators!","text":"unpack_operators!(operators, O, dims, operator_symbols)\n\n\nUnpack the operators from the operator matrix O.\n\n\n\n\n\n","category":"method"},{"location":"manual/Intrusive/#Intrusive-Model-Reduction","page":"Intrusive","title":"Intrusive Model Reduction","text":"","category":"section"},{"location":"manual/Intrusive/#POD","page":"Intrusive","title":"POD","text":"","category":"section"},{"location":"manual/Intrusive/","page":"Intrusive","title":"Intrusive","text":"Proper orthogonal decomposition originated from the analysis of turbulent flows in aerodynamics, and it has become one of the most widespread projection-based model reduction methods. POD reduces the model by projecting it onto a reduced subspace defined to be the span of basis vectors that optimally represent a set of simulation or experimental data. See the original literatures on POD [3], [4], [5].","category":"page"},{"location":"manual/Intrusive/","page":"Intrusive","title":"Intrusive","text":"In POD, we begin by collecting snapshots of state trajectory time series data by simulating the original full model ODE with K timesteps. We define the state snapshot data matrix as follows: ","category":"page"},{"location":"manual/Intrusive/","page":"Intrusive","title":"Intrusive","text":"    mathbfX = beginbmatrix \n               \n        boldsymbolmathbf x(t_1)  mathbfx(t_2)  cdots  mathbfx(t_K) \n               \n    endbmatrix in mathbbR^ntimes K ","category":"page"},{"location":"manual/Intrusive/","page":"Intrusive","title":"Intrusive","text":"More generally, the state snapshot matrix can contain state data from multiple simulations, e.g., from different initial conditions or using different parameters. Let mathbfX = mathbfVSigma W^top denote the singular value decomposition of the state snapshot. To reduce the dimension of the large-scale model, we denote by mathbfV_rin mathbb R^ntimes r the first r ll n columns of mathbf V; this is called the POD basis. Then, we approximate the state mathbfx in the subspace spanned by the POD basis, mathbf x approx mathbf V_r hatmathbf x where hatmathbf xinmathbbR^r is called the reduced state. If we substitute this approximation into a linear-quadratic system and enforce the Galerkin orthogonality condition that the approximation residual be orthogonal to the span of mathbf V_r, we arrive at a POD-Galerkin reduced model of the form","category":"page"},{"location":"manual/Intrusive/","page":"Intrusive","title":"Intrusive","text":"    dothatmathbf x(t) = mathbfhat Ahatmathbf x(t) + hatmathbfH(hatmathbfx(t) otimes hatmathbfx(t))","category":"page"},{"location":"manual/Intrusive/","page":"Intrusive","title":"Intrusive","text":"where the reduced operators are mathbfhatA = mathbfV^top_r mathbfAV_r in mathbbR^rtimes r and  hatmathbfH = mathbfV^top_r mathbfH(mathbfV_r otimes mathbfV_r) in mathbb R^rtimes r^2.","category":"page"},{"location":"manual/Intrusive/#Implementation","page":"Intrusive","title":"Implementation","text":"","category":"section"},{"location":"manual/Intrusive/","page":"Intrusive","title":"Intrusive","text":"The implementation of this corresponds to the following function:","category":"page"},{"location":"manual/Intrusive/","page":"Intrusive","title":"Intrusive","text":"pod","category":"page"},{"location":"manual/Intrusive/#LiftAndLearn.pod","page":"Intrusive","title":"LiftAndLearn.pod","text":"pod(op, Vr, sys_struct; nonredundant_operators)\n\n\nPerform intrusive model reduction using Proper Orthogonal Decomposition (POD). This implementation is liimted to\n\nstate: up to 4th order\ninput: only B matrix\noutput: only C and D matrices\nstate-input-coupling: bilinear \nconstant term: K matrix\n\nArguments\n\nop: operators of the target system \nVr: POD basis\noptions: options for the operator inference\n\nReturn\n\nop_new: new operator projected onto the basis\n\n\n\n\n\n","category":"function"},{"location":"manual/Options/#Options-for-Model-Learning","page":"Options","title":"Options for Model Learning","text":"","category":"section"},{"location":"manual/Options/#General-Setup-Options","page":"Options","title":"General Setup Options","text":"","category":"section"},{"location":"manual/Options/#System-Structure","page":"Options","title":"System Structure","text":"","category":"section"},{"location":"manual/Options/","page":"Options","title":"Options","text":"This package works with model reduction for polynomial systems with affine control taking a generel form of ","category":"page"},{"location":"manual/Options/","page":"Options","title":"Options","text":"dotmathbfx = mathbfA_1mathbfx + mathbfA_2(mathbfxotimesmathbfx) + cdots + mathbfA_n(underbracemathbfxotimescdotsotimesmathbfx_n-texttimes) + sum_i=1^m N_1imathbfxmathbfu_i + cdots + sum_i=1^m N_ki(underbracemathbfxotimescdotsotimesmathbfx_k-texttimes)mathbfu_i + mathbfBmathbfu +mathbfK","category":"page"},{"location":"manual/Options/","page":"Options","title":"Options","text":"note: Current Implementations\nWith the current version we have only implemented up to quartic states and bilinear controls. We plan to implement linear-quadratic controls and other-higher order systems in the future, as these types of systems are observed in many applications. ","category":"page"},{"location":"manual/Options/","page":"Options","title":"Options","text":"For such structures, use SystemStructure struct to define the system.","category":"page"},{"location":"manual/Options/","page":"Options","title":"Options","text":"SystemStructure","category":"page"},{"location":"manual/Options/#LiftAndLearn.SystemStructure","page":"Options","title":"LiftAndLearn.SystemStructure","text":"mutable struct SystemStructure\n\nStructure of the given system.\n\nFields\n\nstate::Union{Array{<:Int,1},Int}: the state variables\ncontrol::Union{Array{<:Int,1},Int}: the control variables\noutput::Union{Array{<:Int,1},Int}: the output variables\ncoupled_input::Union{Array{<:Int,1},Int}: the coupled input variables\ncoupled_output::Union{Array{<:Int,1},Int}: the coupled output variables\nconstant::Int: the constant variables\nconstant_output::Int: the constant output variables\n\nNote\n\nThe variables are represented as integers or arrays of integers.\n\n\n\n\n\n","category":"type"},{"location":"manual/Options/#Variable-info","page":"Options","title":"Variable info","text":"","category":"section"},{"location":"manual/Options/","page":"Options","title":"Options","text":"This structure allows you to input information about the variables in the system. For example, the Fitzhugh-Nagumo system has two main variables: a fast-recovery variable (usually denoted as v) and a slow-activating variable (usually denoted as w). The Fitzhugh-Nagumo equation is given by:","category":"page"},{"location":"manual/Options/","page":"Options","title":"Options","text":"beginalign*\n    fracdvdt = v - fracv^33 - w + I \n    fracdwdt = epsilon (v + a - bw)\nendalign*","category":"page"},{"location":"manual/Options/","page":"Options","title":"Options","text":"Additionally, if we lift this system, it becomes a lifted system of 3 variables. For the VariableStructure you can define the number of unlifted state variables N=2 and the number of lifted state variables Nl=3.","category":"page"},{"location":"manual/Options/","page":"Options","title":"Options","text":"VariableStructure","category":"page"},{"location":"manual/Options/#LiftAndLearn.VariableStructure","page":"Options","title":"LiftAndLearn.VariableStructure","text":"mutable struct VariableStructure\n\nInformation about the system variables.\n\nFields\n\nN::Int64: the number of system variables\nN_lift::Int64: the number of lifted system variables\n\n\n\n\n\n","category":"type"},{"location":"manual/Options/#Data-Info","page":"Options","title":"Data Info","text":"","category":"section"},{"location":"manual/Options/","page":"Options","title":"Options","text":"With the DataStructure struct, the user will define the time-step Delta t and optionally the down-sampling and type of numerical scheme used for the Partial Differential Equation.","category":"page"},{"location":"manual/Options/","page":"Options","title":"Options","text":"DataStructure","category":"page"},{"location":"manual/Options/#LiftAndLearn.DataStructure","page":"Options","title":"LiftAndLearn.DataStructure","text":"mutable struct DataStructure\n\nInformation about the data.\n\nFields\n\nΔt::Float64: the time step or temporal discretization\nDS::Int64: the downsampling rate\nderiv_type::String: the derivative scheme, e.g. \"F\"orward \"E\"uler\n\n\n\n\n\n","category":"type"},{"location":"manual/Options/#Optimization-Settings","page":"Options","title":"Optimization Settings","text":"","category":"section"},{"location":"manual/Options/","page":"Options","title":"Options","text":"This options is only required if you are dealing with the optimization based model reduction methods (e.g., Energy-Preserving Operator Inference) in which you must select some options for the optimization. ","category":"page"},{"location":"manual/Options/","page":"Options","title":"Options","text":"OptimizationSetting","category":"page"},{"location":"manual/Options/#LiftAndLearn.OptimizationSetting","page":"Options","title":"LiftAndLearn.OptimizationSetting","text":"mutable struct OptimizationSetting\n\nInformation about the optimization.\n\nFields\n\nverbose::Bool: enable the verbose output for optimization\ninitial_guess::Bool: use initial guesses for optimization\nmax_iter::Int64: the maximum number of iterations for the optimization\nnonredundant_operators::Bool: use nonredundant operators\nreproject::Bool: use reprojection method for derivative data\nSIGE::Bool: use successive initial guess estimation\nwith_bnds::Bool: add bounds to the variables\nlinear_solver::String: the linear solver to use for optimization\nHSL_lib_path::String: the path to the HSL library\n\n\n\n\n\n","category":"type"},{"location":"manual/Options/#Reprojection","page":"Options","title":"Reprojection","text":"","category":"section"},{"location":"manual/Options/","page":"Options","title":"Options","text":"Reprojection is a sampling scheme used to increase the accuracy of the recovered reduced model. For further details and mathematical proofs refer to [1] and [2].","category":"page"},{"location":"manual/Options/#Successive-Initial-Guess-Estimation-(SIGE)","page":"Options","title":"Successive Initial Guess Estimation (SIGE)","text":"","category":"section"},{"location":"manual/Options/","page":"Options","title":"Options","text":"The SIGE option is used to run the optimization successive from a lower reduced dimension and increasing the dimensions sequentially by using the solution of the lower dimensional operator as the initial guess for the next optimization. For example, you will solve the optimization for r=2 and then use the solution to solve for the optimization of r=3.","category":"page"},{"location":"manual/Options/","page":"Options","title":"Options","text":"# r = 2\noptions.optim.initial_guess = false  # turn off initial guess for the first iteration\nop_tmp = LnL.opinf(Xdata, Vr[:,1:2], options; U=zeros(100), Y=zeros(100), Xdot=Xdotdata)  # compute the first operator\nop[1] = op_tmp  # store the first operator\n\n# r =3\noptions.optim.initial_guess = true  # turn on initial guess for the next step\nop_tmp = LnL.opinf(Xdata, Vr[:,1:3], options; U=zeros(100), Y=zeros(100), Xdot=Xdotdata, IG=LnL.Operators(A=op_tmp.A, F=op_tmp.F)) # compute the second operator\nop[2] = op_tmp","category":"page"},{"location":"manual/Options/#Linear-Solvers","page":"Options","title":"Linear Solvers","text":"","category":"section"},{"location":"manual/Options/","page":"Options","title":"Options","text":"For the optimization we use Ipopt, and for its linear solvers it is possible to use the default solvers like MUMPS and HSL solvers (e.g., ma77, ma86, and ma97).","category":"page"},{"location":"manual/Options/","page":"Options","title":"Options","text":"warning: Warning\nTo use HSL linear solvers you will need to obtain a license from here. Then set the path to the solvers using the option HSL_lib_path:import HSL_jll\nOptimizationSetting.HSL_lib_path = HSL_jll.libhsl_path","category":"page"},{"location":"manual/Options/#Regularization","page":"Options","title":"Regularization","text":"","category":"section"},{"location":"manual/Options/","page":"Options","title":"Options","text":"Using the TikhonovParameter struct you will define the Tikhonov regularization matrix. This will be a diagonal matrix with diagonal entries having different values corresponding to the operators that it is regulating (e.g., linear, quadratic, bilinear). ","category":"page"},{"location":"manual/Options/","page":"Options","title":"Options","text":"The Tikhonov regulated optimization problem is defined as ","category":"page"},{"location":"manual/Options/","page":"Options","title":"Options","text":"textminmathbfDmathbfO^top - dotmathbfhatX^2_F + mathbfGammamathbfO^top^2_F","category":"page"},{"location":"manual/Options/","page":"Options","title":"Options","text":"where mathbfD, mathbfO^top, dothatmathbfX are the data matrix, operator matrix (with minimizers), and time derivative snapshot matrix, respectively. The linear least-squares solution of this becomes","category":"page"},{"location":"manual/Options/","page":"Options","title":"Options","text":"mathbfO^top = (mathbfD^topmathbfD + mathbfGamma^topmathbfGamma)^dag mathbfDdotmathbfhatX","category":"page"},{"location":"manual/Options/","page":"Options","title":"Options","text":"TikhonovParameter","category":"page"},{"location":"manual/Options/#LiftAndLearn.TikhonovParameter","page":"Options","title":"LiftAndLearn.TikhonovParameter","text":"struct TikhonovParameter\n\nTikhonov regularization parameters.\n\nFields\n\nA::Union{Real, AbstractArray{Real}}: the Tikhonov regularization parameter for the linear state operator\nA2::Real: the Tikhonov regularization parameter for the quadratic state operator\nA3::Real: the Tikhonov regularization parameter for the cubic state operator\nA4::Real: the Tikhonov regularization parameter for the quartic state operator\nB::Real: the Tikhonov regularization parameter for the linear input operator\nN::Real: the Tikhonov regularization parameter for the bilinear state-input operator\nC::Real: the Tikhonov regularization parameter for the constant operator\nK::Real: the Tikhonov regularization parameter for the constant output operator\n\n\n\n\n\n","category":"type"},{"location":"manual/Options/#Model-Reduction-Specific-Options","page":"Options","title":"Model Reduction Specific Options","text":"","category":"section"},{"location":"manual/Options/","page":"Options","title":"Options","text":"These options are all specific to each solution method of Operator Inference. All of the options below are a subtype of AbstractOption.","category":"page"},{"location":"manual/Options/","page":"Options","title":"Options","text":"LiftAndLearn.AbstractOption","category":"page"},{"location":"manual/Options/#LiftAndLearn.AbstractOption","page":"Options","title":"LiftAndLearn.AbstractOption","text":"AbstractOption\n\nAbstract type for the options.\n\n\n\n\n\n","category":"type"},{"location":"manual/Options/#Standard-Operator-Inference","page":"Options","title":"Standard Operator Inference","text":"","category":"section"},{"location":"manual/Options/","page":"Options","title":"Options","text":"This option is required when using the standard Operator Inference method.","category":"page"},{"location":"manual/Options/","page":"Options","title":"Options","text":"LSOpInfOption","category":"page"},{"location":"manual/Options/#LiftAndLearn.LSOpInfOption","page":"Options","title":"LiftAndLearn.LSOpInfOption","text":"mutable struct LSOpInfOption <: LiftAndLearn.AbstractOption\n\nStandard least-squares Operator Inference.\n\nFields\n\nmethod::Symbol: the name of the method\nsystem::SystemStructure: the system structure\nvars::VariableStructure: the system variables\ndata::DataStructure: the data\noptim::OptimizationSetting: the optimization settings\nλ::TikhonovParameter: the Tikhonov regularization parameters\nwith_tol::Bool: the option to use tolerance for the least square pseudo inverse\nwith_reg::Bool: the option to use Tikhonov regularization\npinv_tol::Real: the tolerance for the least square pseudo inverse\n\n\n\n\n\n","category":"type"},{"location":"manual/Options/","page":"Options","title":"Options","text":"note: Note\nThe option with_tol turns on/off the settings to truncate ill-posed singular values with order of pinv_tol.","category":"page"},{"location":"manual/Options/#Energy-Preserving-Operator-Inference-Options","page":"Options","title":"Energy-Preserving Operator Inference Options","text":"","category":"section"},{"location":"manual/Options/","page":"Options","title":"Options","text":"The three options below are for the energy-preserving Operator Inference approaches: hard equality constraint, soft inequality constraint, and penalty. For details of each parameter please check out the documentation of EP-OpInf Manual","category":"page"},{"location":"manual/Options/","page":"Options","title":"Options","text":"EPHECOpInfOption\nEPSICOpInfOption\nEPPOpInfOption","category":"page"},{"location":"manual/Options/#LiftAndLearn.EPHECOpInfOption","page":"Options","title":"LiftAndLearn.EPHECOpInfOption","text":"mutable struct EPHECOpInfOption <: LiftAndLearn.AbstractOption\n\nEnergy-Preserving Hard Equality Constraint Operator Inference.\n\nFields\n\nmethod::Symbol: the name of the method\nsystem::SystemStructure: the system structure\nvars::VariableStructure: the system variables\ndata::DataStructure: the data\noptim::OptimizationSetting: the optimization settings\nλ_lin::Real: the Tikhonov regularization parameter for linear state operator\nλ_quad::Real: the Tikhonov regularization parameter for quadratic state operator\nlinear_operator_bounds::Tuple{Float64, Float64}: the bounds for the linear operator\nquad_operator_bounds::Tuple{Float64, Float64}: the bounds for the quadratic operator\n\n\n\n\n\n","category":"type"},{"location":"manual/Options/#LiftAndLearn.EPSICOpInfOption","page":"Options","title":"LiftAndLearn.EPSICOpInfOption","text":"mutable struct EPSICOpInfOption <: LiftAndLearn.AbstractOption\n\nEnergy-Preserving Soft Inequality Constraint Operator Inference.\n\nFields\n\nmethod::Symbol: the name of the method\nsystem::SystemStructure: the system structure\nvars::VariableStructure: the system variables\ndata::DataStructure: the data\noptim::OptimizationSetting: the optimization settings\nλ_lin::Real: the Tikhonov regularization parameter for linear state operator\nλ_quad::Real: the Tikhonov regularization parameter for quadratic state operator\nϵ::Real: soft constraint radius\nlinear_operator_bounds::Tuple{Float64, Float64}: the bounds for the linear operator\nquad_operator_bounds::Tuple{Float64, Float64}: the bounds for the quadratic operator\n\n\n\n\n\n","category":"type"},{"location":"manual/Options/#LiftAndLearn.EPPOpInfOption","page":"Options","title":"LiftAndLearn.EPPOpInfOption","text":"mutable struct EPPOpInfOption <: LiftAndLearn.AbstractOption\n\nEnergy-Preserving Penalty Operator Inference.\n\nFields\n\nmethod::Symbol: the name of the method\nsystem::SystemStructure: the system structure\nvars::VariableStructure: the system variables\ndata::DataStructure: the data\noptim::OptimizationSetting: the optimization settings\nλ_lin::Real: the Tikhonov regularization parameter for linear state operator\nλ_quad::Real: the Tikhonov regularization parameter for quadratic state operator\nα::Float64: the weight for the energy-preserving term in the cost function\nlinear_operator_bounds::Tuple{Float64, Float64}: the bounds for the linear operator\nquad_operator_bounds::Tuple{Float64, Float64}: the bounds for the quadratic operator\n\n\n\n\n\n","category":"type"},{"location":"manual/Utilities/#Utilities","page":"Utilities","title":"Utilities","text":"","category":"section"},{"location":"manual/Utilities/#System-operators","page":"Utilities","title":"System operators","text":"","category":"section"},{"location":"manual/Utilities/","page":"Utilities","title":"Utilities","text":"The struct Operators provides a convenient way to organize and store the full and reduced operators.","category":"page"},{"location":"manual/Utilities/","page":"Utilities","title":"Utilities","text":"Operators","category":"page"},{"location":"manual/Utilities/#LiftAndLearn.Operators","page":"Utilities","title":"LiftAndLearn.Operators","text":"mutable struct Operators\n\nOrganize the operators of the system in a structure. The operators currently  supported are up to second order.\n\nFields\n\nA: linear state operator\nB: linear input operator\nC: linear output operator\nD: linear input-output coupling operator\nA2: quadratic state operator with redundancy\nA3: cubic state operator with redundancy\nA4: quartic state operator with redundancy\nA2u: quadratic state operator with no redundancy\nA3u: cubic state operator with no redundancy\nA4u: quartic state operator with no redundancy\nA2t: quadratic state operator with redundancy in 3-dim tensor form\nN: bilinear (state-input) operator\nK: constant operator\nf: nonlinear function operator f(x,u)\ndims: dimensions of the operators\n\nNote\n\nCurrently only supports\nstate: up to 4th order\ninput: only B matrix\noutput: only C and D matrices\nstate-input-coupling: bilinear \nconstant term: K matrix\nnonlinearity: f(x,u)\n\n\n\n\n\n","category":"type"},{"location":"manual/Utilities/#Math-Operations","page":"Utilities","title":"Math Operations","text":"","category":"section"},{"location":"manual/Utilities/","page":"Utilities","title":"Utilities","text":"This package heavily relies on the following math operations","category":"page"},{"location":"manual/Utilities/","page":"Utilities","title":"Utilities","text":"Kronecker Products otimes\nVectorization textvec(cdot)","category":"page"},{"location":"manual/Utilities/","page":"Utilities","title":"Utilities","text":"For Kronecker products we are very thankful to the Kronecker.jl package allowing fast Kronecker product arithmetic. We also, use what is called the unique Kronecker product which eliminates the redundant terms arising from the symmetry of Kronecker products. For details on the unique Kronecker product please refer to the package UniqueKronecker.jl.","category":"page"},{"location":"manual/Utilities/#Partial-Differential-Equation-Models","page":"Utilities","title":"Partial Differential Equation Models","text":"","category":"section"},{"location":"manual/Utilities/","page":"Utilities","title":"Utilities","text":"Please also refer to the package PolynomialModelReductionDataset.jl providing a suite of polynomial-based systems arising from PDEs. We extensively use these models in our examples.","category":"page"},{"location":"manual/nonintrusive/EPOpInf/#Energy-Preserving-Operator-Inference","page":"Energy Preserving","title":"Energy-Preserving Operator Inference","text":"","category":"section"},{"location":"manual/nonintrusive/EPOpInf/","page":"Energy Preserving","title":"Energy Preserving","text":"This is a modified version of Operator Inference. Namely, it solves a constrained optimization for the reduced operators.","category":"page"},{"location":"manual/nonintrusive/EPOpInf/#Energy-Preserving-Quadratic-Nonlinearities-in-PDEs","page":"Energy Preserving","title":"Energy Preserving Quadratic Nonlinearities in PDEs","text":"","category":"section"},{"location":"manual/nonintrusive/EPOpInf/","page":"Energy Preserving","title":"Energy Preserving","text":"We consider an n-dimensional ordinary differential equation (ODE) which is linear and quadratic in the state mathbf x. Such an ODE often arises from spatially discretizing a PDE and is given by","category":"page"},{"location":"manual/nonintrusive/EPOpInf/","page":"Energy Preserving","title":"Energy Preserving","text":"    dotmathbf x(t) = mathbf A mathbf x(t) + mathbf H left(mathbf x(t) otimes mathbf x(t)right) ","category":"page"},{"location":"manual/nonintrusive/EPOpInf/","page":"Energy Preserving","title":"Energy Preserving","text":"where mathbf x(t) in mathbbR^n is the system state vector over t in 0 T_textfinal, and otimes denotes the Kronecker product. The operators mathbf A in mathbbR^ntimes n and mathbf H in mathbbR^ntimes n^2 are the linear and quadratic operators, respectively. In our setting, n is large, so simulating the system is computationally expensive.","category":"page"},{"location":"manual/nonintrusive/EPOpInf/","page":"Energy Preserving","title":"Energy Preserving","text":"The quadratic operator mathbf H is called `energy-preserving' if","category":"page"},{"location":"manual/nonintrusive/EPOpInf/","page":"Energy Preserving","title":"Energy Preserving","text":"    langle mathbf x mathbf H (mathbf x otimes mathbf x)rangle = mathbf x^top mathbf H(mathbf x otimes mathbf x) = 0 qquad text for all  mathbf x in mathbb R^n","category":"page"},{"location":"manual/nonintrusive/EPOpInf/","page":"Energy Preserving","title":"Energy Preserving","text":"This condition is derived by setting the quadratic term in the time derivative of the energy, frac12mathbfx^2, to zero.","category":"page"},{"location":"manual/nonintrusive/EPOpInf/#Energy-Preserving-Operator-Inference-(EP-OpInf)","page":"Energy Preserving","title":"Energy-Preserving Operator Inference (EP-OpInf)","text":"","category":"section"},{"location":"manual/nonintrusive/EPOpInf/","page":"Energy Preserving","title":"Energy Preserving","text":"To impose this energy-preserving structure on the operator, we propose EP-OpInf. For this method, we incorporate the constraint into the standard OpInf optimization and formulate a constrained minimization as follows:","category":"page"},{"location":"manual/nonintrusive/EPOpInf/","page":"Energy Preserving","title":"Energy Preserving","text":"    textbfEP-OpInf qquad \n    min_mathbfOinmathbb R^rtimes(r+r^2) mathbfDmathbfO^top - dothatmathbfX_F^2 quad text subject to  quad hat h_ijk + hat h_jik + hat h_kij = 0 quad  1 leq ijk leq r  ","category":"page"},{"location":"manual/nonintrusive/EPOpInf/","page":"Energy Preserving","title":"Energy Preserving","text":"note: Multiple Optimization Methods\nFor EP-OpInf, we implement multiple variations of the EP constraint.A hard equality constraint.\nA soft inequality constraint.\nA Penalty method where the constraint is indirectly applied to the problem.Note that the hard equality constraint is the enforces the constraint in the strictest manner and the penalty method is the weakest. The inequality constraint lies between the two. However, depending on the parameter settings the constraint violations may vary between the inequality and penalty methods.","category":"page"},{"location":"manual/nonintrusive/EPOpInf/","page":"Energy Preserving","title":"Energy Preserving","text":"epopinf","category":"page"},{"location":"manual/nonintrusive/EPOpInf/#LiftAndLearn.epopinf","page":"Energy Preserving","title":"LiftAndLearn.epopinf","text":"epopinf(X, Vn, options; U, Xdot, IG)\n\n\nEnergy-preserving Operator Inference (EPOpInf) optimization problem.\n\n\n\n\n\n","category":"function"},{"location":"manual/nonintrusive/LS/#Standard-Operator-Inference-(OpInf)","page":"Standard OpInf","title":"Standard Operator Inference (OpInf)","text":"","category":"section"},{"location":"manual/nonintrusive/LS/#Theory","page":"Standard OpInf","title":"Theory","text":"","category":"section"},{"location":"manual/nonintrusive/LS/","page":"Standard OpInf","title":"Standard OpInf","text":"If we consider a linear-quadratic system, the goal of Operator Inference is to non-intrusively obtain a reduced model of the form","category":"page"},{"location":"manual/nonintrusive/LS/","page":"Standard OpInf","title":"Standard OpInf","text":"    dothatmathbf x(t) = hatmathbf Ahatmathbf x(t) + hatmathbfH(hatmathbfx(t) otimes hatmathbfx(t))","category":"page"},{"location":"manual/nonintrusive/LS/","page":"Standard OpInf","title":"Standard OpInf","text":"To do so, we will fit reduced operators hatmathbfA and hatmathbfH to the reduced data in a least-squares sense. In addition to the state trajectory data ","category":"page"},{"location":"manual/nonintrusive/LS/","page":"Standard OpInf","title":"Standard OpInf","text":"    mathbfX = beginbmatrix \n               \n        boldsymbolmathbf x(t_1)  mathbfx(t_2)  cdots  mathbfx(t_K) \n               \n    endbmatrix in mathbbR^ntimes K ","category":"page"},{"location":"manual/nonintrusive/LS/","page":"Standard OpInf","title":"Standard OpInf","text":"we also require paired state time derivative data: ","category":"page"},{"location":"manual/nonintrusive/LS/","page":"Standard OpInf","title":"Standard OpInf","text":"beginaligned\n    dotmathbfX = beginbmatrix\n              \n        dotmathbfx(t_1)  dotmathbfx(t_2)  cdots  dotmathbfx(t_K) \n              \n    endbmatrixinmathbbR^ntimes K quad text where  \n    dotmathbf x(t_i)=mathbf Amathbf x(t_i)+ mathbf H (mathbf x(t_i) otimes mathbf x(t_i))\nendaligned","category":"page"},{"location":"manual/nonintrusive/LS/","page":"Standard OpInf","title":"Standard OpInf","text":"This time derivative data can come directly from the simulation of the high-dimensional ODE or can be approximated numerically from the snapshot data. We use the POD basis mathbfV_r to compute reduced state and time derivative data as follows: let hatmathbfx_i = mathbfV_r^topmathbfx(t_i) as an ansatz, and dothatmathbfx_i = mathbfV_r^topdotmathbfx(t_i) for i=1ldotsK. Then, define","category":"page"},{"location":"manual/nonintrusive/LS/","page":"Standard OpInf","title":"Standard OpInf","text":"    hattextbf X = beginbmatrix\n               \n        boldsymbolhatmathbf x_1  boldsymbol hatmathbf x_2  cdots  boldsymbolhatmathbf x_K \n               \n    endbmatrix in mathbb R^rtimes K qquad textand qquad \n    dothattextbf X = beginbmatrix\n               \n        dothatmathbf x_1  dothatmathbf x_2  cdots  dothatmathbf x_K \n               \n    endbmatrix in mathbb R^rtimes K","category":"page"},{"location":"manual/nonintrusive/LS/","page":"Standard OpInf","title":"Standard OpInf","text":"Additionally, we define the matrix formed by the quadratic terms of the state data","category":"page"},{"location":"manual/nonintrusive/LS/","page":"Standard OpInf","title":"Standard OpInf","text":"    hattextbf X_otimes = beginbmatrix\n               \n        (hatmathbf x_1 otimes hatmathbf x_1)  (hatmathbf x_2 otimes hatmathbf x_2)  cdots  (hatmathbf x_K otimes hatmathbf x_K) \n               \n    endbmatrix in mathbb R^r^2times K","category":"page"},{"location":"manual/nonintrusive/LS/","page":"Standard OpInf","title":"Standard OpInf","text":"This allows us to formulate the following minimization for finding the reduced operators hatmathbfA and hatmathbfH:   ","category":"page"},{"location":"manual/nonintrusive/LS/","page":"Standard OpInf","title":"Standard OpInf","text":"    textbfStandard OpInf qquad \n    min_mathbfhatAinmathbb R^rtimes rhatmathbfHinmathbb R^rtimes r^2 sum_i=1^K left dothatmathbf x_i - mathbfhatAhatmathbfx_i - hatmathbf H(hatmathbf x_i otimes hatmathbf x_i) right ^2_2 = min_mathbfOinmathbb R^rtimes(r+r^2) mathbfDmathbfO^top - dothatmathbf X_F^2","category":"page"},{"location":"manual/nonintrusive/LS/","page":"Standard OpInf","title":"Standard OpInf","text":"where mathbf D = hatmathbf X^top hatmathbf X_otimes^top in mathbb R^Ktimes(r+r^2) and mathbf O = mathbfhat A hatmathbf H in mathbb R^rtimes(r+r^2).","category":"page"},{"location":"manual/nonintrusive/LS/#Implementation","page":"Standard OpInf","title":"Implementation","text":"","category":"section"},{"location":"manual/nonintrusive/LS/","page":"Standard OpInf","title":"Standard OpInf","text":"In this package we implement the standard OpInf along with Tikhonov regularized version with the function opinf.","category":"page"},{"location":"manual/nonintrusive/LS/","page":"Standard OpInf","title":"Standard OpInf","text":"There are many things going on under the hood when the function: ","category":"page"},{"location":"manual/nonintrusive/LS/","page":"Standard OpInf","title":"Standard OpInf","text":"constructing the data matrix\nconstruting the Tikhonov matrix\nreprojection","category":"page"},{"location":"manual/nonintrusive/LS/","page":"Standard OpInf","title":"Standard OpInf","text":"But all of those operations are taken care of automatically. For full details please see the source code.","category":"page"},{"location":"manual/nonintrusive/LS/","page":"Standard OpInf","title":"Standard OpInf","text":"opinf","category":"page"},{"location":"manual/nonintrusive/LS/#LiftAndLearn.opinf","page":"Standard OpInf","title":"LiftAndLearn.opinf","text":"opinf(X::AbstractArray, Vn::AbstractArray, options::AbstractOption; \n    U::AbstractArray=zeros(1,1), Y::AbstractArray=zeros(1,1),\n    Xdot::AbstractArray=[]) → op::Operators\n\nInfer the operators with derivative data given. NOTE: Make sure the data is  constructed such that the row is the state vector and the column is the time.\n\nArguments\n\nX::AbstractArray: state data matrix\nVn::AbstractArray: POD basis\noptions::AbstractOption: options for the operator inference defined by the user\nU::AbstractArray: input data matrix\nY::AbstractArray: output data matix\nXdot::AbstractArray: derivative data matrix\n\nReturns\n\nop::Operators: inferred operators\n\n\n\n\n\nopinf(X::AbstractArray, Vn::AbstractArray, full_op::Operators, options::AbstractOption;\n    U::AbstractArray=zeros(1,1), Y::AbstractArray=zeros(1,1)) → op::Operators\n\nInfer the operators with reprojection method (dispatch). NOTE: Make sure the data is constructed such that the row is the state vector and the column is the time.\n\nArguments\n\nX::AbstractArray: state data matrix\nVn::AbstractArray: POD basis\nfull_op::Operators: full order model operators\noptions::AbstractOption: options for the operator inference defined by the user\nU::AbstractArray: input data matrix\nY::AbstractArray: output data matix\nreturn_derivative::Bool=false: return the derivative matrix (or residual matrix)\n\nReturns\n\nop::Operators: inferred operators\n\n\n\n\n\nopinf(W::AbstractArray, Vn::AbstractArray, lm::lifting, full_op::Operators,\n        options::AbstractOption; U::AbstractArray=zeros(1,1), \n        Y::AbstractArray=zeros(1,1), IG::Operators=Operators()) → op::Operators\n\nInfer the operators for Lift And Learn for reprojected data (dispatch). NOTE: make sure that the data is constructed such that the row dimension is the state dimension and the column dimension is the time dimension. \n\nArguments\n\nW::AbstractArray: state data matrix\nVn::AbstractArray: POD basis\nlm::lifting: struct of the lift map\nfull_op::Operators: full order model operators\noptions::AbstractOption: options for the operator inference defined by the user\nU::AbstractArray: input data matrix\nY::AbstractArray: output data matix\n\nReturns\n\nop::Operators: inferred operators\n\nNote\n\nYou can opt to use the unlifted version of the opinf function instead of this dispatch if it is not necessary to reproject the data.\n\n\n\n\n\n","category":"function"},{"location":"paper/#Paper","page":"Paper Reference","title":"Paper","text":"","category":"section"},{"location":"paper/","page":"Paper Reference","title":"Paper Reference","text":"Below you have a list of publications referenced in this work.","category":"page"},{"location":"paper/","page":"Paper Reference","title":"Paper Reference","text":"W. I. Uy, Y. Wang, Y. Wen and B. Peherstorfer. Active Operator Inference for Learning Low-Dimensional Dynamical-System Models from Noisy Data. SIAM Journal on Scientific Computing 45, A1462–A1490 (2023).\n\n\n\nB. Peherstorfer. Sampling Low-Dimensional Markovian Dynamics for Preasymptotically Recovering Reduced Models from Data with Operator Inference. SIAM Journal on Scientific Computing 42, A3489–A3515 (2020).\n\n\n\nJ. L. Lumley. The structure of inhomogeneous turbulent flows. Atmospheric turbulence and radio wave propagation, 166–178 (1967).\n\n\n\nL. Sirovich. Turbulence and the dynamics of coherent structures. I. Coherent structures. Quarterly of applied mathematics 45, 561–571 (1987).\n\n\n\nG. Berkooz, P. Holmes and J. L. Lumley. The proper orthogonal decomposition in the analysis of turbulent flows. Annual review of fluid mechanics 25, 539–575 (1993).\n\n\n\n","category":"page"},{"location":"#LiftAndLearn","page":"Home","title":"LiftAndLearn","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"LiftAndLearn.jl is an implementation of the Lift and Learn as well as the operator inference algorithm proposed in the papers listed in Key References.","category":"page"},{"location":"#Operator-Inference-(OpInf)","page":"Home","title":"Operator Inference (OpInf)","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Operator Inference is a scientific machine-learning framework used in data-driven modeling of dynamical systems that aims to learn the governing equations or operators from observed data without explicit knowledge of the underlying physics or dynamics (but with some information such as the structure, e.g., linear, quadratic, bilinear, etc.). To know more about OpInf, please refer to these resources by Willcox Research Group and ACE Lab. Or you can head over to the documentation page of this package about OpInf.","category":"page"},{"location":"#Lift-and-Learn-(LnL)","page":"Home","title":"Lift and Learn (LnL)","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Lift and Learn is a physics-informed method for learning low-dimensional models for large-scale dynamical systems. Lifting refers to the transformation of the original nonlinear system to a linear, quadratic, bilinear, or polynomial system by mapping the original state space to a new space with additional auxiliary variables. After lifting the system to a more approachable form, we can learn a reduced model using the OpInf approach. For more info, head over to the documentation on LnL.","category":"page"},{"location":"#Requirements","page":"Home","title":"Requirements","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"julia versions 1.8.5 >\nWe use Ipopt for the optimization (e.g., EP-OpInf)\nThis requires additional proprietary linear-solvers including ma86 and ma97. \nYou can run the code without it by changing the options. By default Ipopt will use MUMPS but we recommend you obtain and download HSL_jll.jl. You can find the instructions here.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To use LiftAndLearn, install Julia, then at the Julia REPL, type:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"LiftAndLearn\")\nusing LiftAndLearn","category":"page"},{"location":"#Features","page":"Home","title":"Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Features included in this package are the following:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Intrusive model reduction using Proper Orthogonal Decomposition (POD)\nNon-intrusive model reduction using the standard Operator Inference\nNon-intrusive model reduction for non-polynomial systems using Lift And Learn\nPhysics-informed Operator Inference approaches\nEnergy-preserving\nMore to come in the future ...","category":"page"},{"location":"","page":"Home","title":"Home","text":"note: Note\nWe are actively working to incorporate new features into this package.","category":"page"},{"location":"#Examples","page":"Home","title":"Examples","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you wish to give this package a try see our Jupyter Notebook examples, where you will find a variety of examples:","category":"page"},{"location":"","page":"Home","title":"Home","text":"1-dimensional heat equation\nViscous Burgers' equation\nFitzHugh-Nagumo equation\nKuramoto-Sivashinksy equation (chaotic system)","category":"page"},{"location":"","page":"Home","title":"Home","text":"If you prefer running scripts rather then notebooks, then see the example scripts.","category":"page"},{"location":"#Contributing","page":"Home","title":"Contributing","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you find any bugs or issues please follow the instructions:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Open an issue with clear explanation of bug. Recommended to have minimal reproduction example.\nIf you have patched the bug on your own, then create a pull request.\nFor further inquiries please contact tkoike3@gatech.edu.","category":"page"},{"location":"#License","page":"Home","title":"License","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The source code is distributed under MIT License.","category":"page"},{"location":"#Key-References","page":"Home","title":"Key References","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"(1) Peherstorfer, B. and Willcox, K.  Data-driven operator inference for non-intrusive projection-based model reduction. Computer Methods in Applied Mechanics and Engineering, 306:196-215, 2016. (Download)","category":"page"},{"location":"","page":"Home","title":"Home","text":"@article{Peherstorfer16DataDriven,\n    title   = {Data-driven operator inference for nonintrusive projection-based model reduction},\n    author  = {Peherstorfer, B. and Willcox, K.},\n    journal = {Computer Methods in Applied Mechanics and Engineering},\n    volume  = {306},\n    pages   = {196-215},\n    year    = {2016},\n}","category":"page"},{"location":"","page":"Home","title":"Home","text":"(2) Qian, E., Kramer, B., Marques, A., and Willcox, K.  Transform & Learn: A data-driven approach to nonlinear model reduction. In the AIAA Aviation 2019 Forum, June 17-21, Dallas, TX. (Download)","category":"page"},{"location":"","page":"Home","title":"Home","text":"@inbook{QKMW2019aviation,\n    author = {Qian, E. and Kramer, B. and Marques, A. N. and Willcox, K. E.},\n    title = {Transform \\&amp; Learn: A data-driven approach to nonlinear model reduction},\n    booktitle = {AIAA Aviation 2019 Forum},\n    doi = {10.2514/6.2019-3707},\n    URL = {https://arc.aiaa.org/doi/abs/10.2514/6.2019-3707},\n    eprint = {https://arc.aiaa.org/doi/pdf/10.2514/6.2019-3707}\n}","category":"page"},{"location":"","page":"Home","title":"Home","text":"(3) Qian, E., Kramer, B., Peherstorfer, B., and Willcox, K. Lift & Learn: Physics-informed machine learning for large-scale nonlinear dynamical systems, Physica D: Nonlinear Phenomena, 2020.","category":"page"},{"location":"","page":"Home","title":"Home","text":"@article{qian2020lift,\n    title={Lift \\& {L}earn: {P}hysics-informed machine learning for large-scale nonlinear dynamical systems},\n    author={Qian, E. and Kramer, B. and Peherstorfer, B. and Willcox, K.},\n    journal={Physica D: Nonlinear Phenomena},\n    volume={406},\n    pages={132401},\n    year={2020},\n    publisher={Elsevier}\n}","category":"page"},{"location":"","page":"Home","title":"Home","text":"(4) Qian, E., Farcas, I.-G., and Willcox, K. Reduced operator inference for nonlinear partial differential equations, SIAM Journal of Scientific Computing, 2022.","category":"page"},{"location":"","page":"Home","title":"Home","text":"@article{doi:10.1137/21M1393972,\n    author = {Qian, Elizabeth and Farca\\c{s}, Ionu\\c{t}-Gabriel and Willcox, Karen},\n    title = {Reduced Operator Inference for Nonlinear Partial Differential Equations},\n    journal = {SIAM Journal on Scientific Computing},\n    volume = {44},\n    number = {4},\n    pages = {A1934-A1959},\n    year = {2022},\n    doi = {10.1137/21M1393972},\n    URL = {https://doi.org/10.1137/21M1393972},\n    eprint = {https://doi.org/10.1137/21M1393972},\n}","category":"page"},{"location":"","page":"Home","title":"Home","text":"(5) Koike, T., Qian, E. Energy-Preserving Reduced Operator Inference for Efficient Design and Control, AIAA SCITECH 2024 Forum. 2024.","category":"page"},{"location":"","page":"Home","title":"Home","text":"@inproceedings{koike2024energy,\n  title={Energy-Preserving Reduced Operator Inference for Efficient Design and Control},\n  author={Koike, Tomoki and Qian, Elizabeth},\n  booktitle={AIAA SCITECH 2024 Forum},\n  pages={1012},\n  year={2024},\n  doi={https://doi.org/10.2514/6.2024-1012}\n}","category":"page"},{"location":"manual/nonintrusive/LnL/#Lift-And-Learn","page":"Lift And Learn","title":"Lift And Learn","text":"","category":"section"},{"location":"manual/nonintrusive/LnL/#Overview","page":"Lift And Learn","title":"Overview","text":"","category":"section"},{"location":"manual/nonintrusive/LnL/","page":"Lift And Learn","title":"Lift And Learn","text":"The Lift and Learn method offers a transformative approach to model reduction in nonlinear dynamical systems, especially those not conforming to polynomial structures. ","category":"page"},{"location":"manual/nonintrusive/LnL/#Lifting-Process","page":"Lift And Learn","title":"Lifting Process","text":"","category":"section"},{"location":"manual/nonintrusive/LnL/","page":"Lift And Learn","title":"Lift And Learn","text":"This method introduces an auxiliary variable, effectively 'lifting' the system into a polynomial framework. This direct transformation contrasts with Koopman theory's approximation approach, streamlining the representation of complex dynamics.","category":"page"},{"location":"manual/nonintrusive/LnL/#Operator-Inference","page":"Lift And Learn","title":"Operator Inference","text":"","category":"section"},{"location":"manual/nonintrusive/LnL/","page":"Lift And Learn","title":"Lift And Learn","text":"After lifting, the system is compatible with the Operator Inference scheme, facilitating the discovery of simplified models through data-driven techniques. This phase is instrumental in deriving interpretable and efficient reduced-order models from intricate dynamical systems.","category":"page"},{"location":"manual/nonintrusive/LnL/","page":"Lift And Learn","title":"Lift And Learn","text":"For further details on lifting, please see lifting.","category":"page"}]
}
