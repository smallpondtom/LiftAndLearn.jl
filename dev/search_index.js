var documenterSearchIndex = {"docs":
[{"location":"manual/Options/#Options-for-Model-Learning","page":"Options","title":"Options for Model Learning","text":"","category":"section"},{"location":"manual/Options/#General-Setup-Options","page":"Options","title":"General Setup Options","text":"","category":"section"},{"location":"manual/Options/#System-Structure","page":"Options","title":"System Structure","text":"","category":"section"},{"location":"manual/Options/","page":"Options","title":"Options","text":"This package works with model reduction for polynomial systems with affine control taking a generel form of ","category":"page"},{"location":"manual/Options/","page":"Options","title":"Options","text":"dotmathbfx = mathbfA_1mathbfx + mathbfA_2(mathbfxotimesmathbfx) + cdots + mathbfA_n(underbracemathbfxotimescdotsotimesmathbfx_n-texttimes) + sum_i=1^m N_1imathbfxmathbfu_i + cdots + sum_i=1^m N_ki(underbracemathbfxotimescdotsotimesmathbfx_k-texttimes)mathbfu_i + mathbfBmathbfu +mathbfK","category":"page"},{"location":"manual/Options/","page":"Options","title":"Options","text":"note: Current Implementations\nWith the current version we have only implemented up to quadratic states and bilinear controls. We plan to implement cubic states (i.e., mathbfxotimesmathbfxotimesmathbfx) and linear-quadratic controls in the future, as these types of systems are observed in many applications. ","category":"page"},{"location":"manual/Options/","page":"Options","title":"Options","text":"For such structures, use sys_struct struct to define the system.","category":"page"},{"location":"manual/Options/","page":"Options","title":"Options","text":"sys_struct","category":"page"},{"location":"manual/Options/#LiftAndLearn.sys_struct","page":"Options","title":"LiftAndLearn.sys_struct","text":"mutable struct sys_struct\n\nStructure of the given system.\n\nFields\n\nis_lin::Bool: the system is linear\nis_quad::Bool: the system is quadratic\nis_bilin::Bool: the system is bilinear\nhas_control::Bool: the system has control inputs\nhas_output::Bool: the system has output\nhas_const::Bool: the system has a constant operator\nhas_funcOp::Bool: the system has a functional operator for ODE\nis_lifted::Bool: the system is lifted\n\n\n\n\n\n","category":"type"},{"location":"manual/Options/#Variable-info","page":"Options","title":"Variable info","text":"","category":"section"},{"location":"manual/Options/","page":"Options","title":"Options","text":"This structure allows you to input information about the variables in the system. For example, the Fitzhugh-Nagumo system has two main variables: a fast-recovery variable (usually denoted as v) and a slow-activating variable (usually denoted as w). The Fitzhugh-Nagumo equation is given by:","category":"page"},{"location":"manual/Options/","page":"Options","title":"Options","text":"beginalign*\n    fracdvdt = v - fracv^33 - w + I \n    fracdwdt = epsilon (v + a - bw)\nendalign*","category":"page"},{"location":"manual/Options/","page":"Options","title":"Options","text":"Additionally, if we lift this system, it becomes a lifted system of 3 variables. For the var you can define the number of unlifted state variables N=2 and the number of lifted state variables Nl=3.","category":"page"},{"location":"manual/Options/","page":"Options","title":"Options","text":"vars","category":"page"},{"location":"manual/Options/#LiftAndLearn.vars","page":"Options","title":"LiftAndLearn.vars","text":"mutable struct vars\n\nInformation about the system variables.\n\nFields\n\nN::Int64: the number of system variables\nN_lift::Int64: the number of lifted system variables\n\n\n\n\n\n","category":"type"},{"location":"manual/Options/#Data-Info","page":"Options","title":"Data Info","text":"","category":"section"},{"location":"manual/Options/","page":"Options","title":"Options","text":"With the data struct, the user will define the time-step Delta t and optionally the down-sampling and type of numerical scheme used for the Partial Differential Equation.","category":"page"},{"location":"manual/Options/","page":"Options","title":"Options","text":"data","category":"page"},{"location":"manual/Options/#LiftAndLearn.data","page":"Options","title":"LiftAndLearn.data","text":"mutable struct data\n\nInformation about the data.\n\nFields\n\nΔt::Float64: the time step or temporal discretization\nDS::Int64: the downsampling rate\nderiv_type::String: the derivative scheme, e.g. \"F\"orward \"E\"uler\n\n\n\n\n\n","category":"type"},{"location":"manual/Options/#Optimization-Settings","page":"Options","title":"Optimization Settings","text":"","category":"section"},{"location":"manual/Options/","page":"Options","title":"Options","text":"This options is only required if you are dealing with the optimization based model reduction methods (e.g., Energy-Preserving Operator Inference) in which you must select some options for the optimization. ","category":"page"},{"location":"manual/Options/","page":"Options","title":"Options","text":"opt_settings","category":"page"},{"location":"manual/Options/#LiftAndLearn.opt_settings","page":"Options","title":"LiftAndLearn.opt_settings","text":"mutable struct opt_settings\n\nInformation about the optimization.\n\nFields\n\nverbose::Bool: enable the verbose output for optimization\ninitial_guess::Bool: use initial guesses for optimization\nmax_iter::Int64: the maximum number of iterations for the optimization\nwhich_quad_term::String: choose main quadratic operator (H or F) to use for computation\nreproject::Bool: use reprojection method for derivative data\nSIGE::Bool: use successive initial guess estimation\nwith_bnds::Bool: add bounds to the variables\nlinear_solver::String: the linear solver to use for optimization\nHSL_lib_path::String: the path to the HSL library\n\n\n\n\n\n","category":"type"},{"location":"manual/Options/#Reprojection","page":"Options","title":"Reprojection","text":"","category":"section"},{"location":"manual/Options/","page":"Options","title":"Options","text":"Reprojection is a sampling scheme used to increase the accuracy of the recovered reduced model. For further details and mathematical proofs refer to [1] and [2].","category":"page"},{"location":"manual/Options/#Successive-Initial-Guess-Estimation-(SIGE)","page":"Options","title":"Successive Initial Guess Estimation (SIGE)","text":"","category":"section"},{"location":"manual/Options/","page":"Options","title":"Options","text":"The SIGE option is used to run the optimization successive from a lower reduced dimension and increasing the dimensions sequentially by using the solution of the lower dimensional operator as the initial guess for the next optimization. For example, you will solve the optimization for r=2 and then use the solution to solve for the optimization of r=3.","category":"page"},{"location":"manual/Options/","page":"Options","title":"Options","text":"# r = 2\noptions.optim.initial_guess = false  # turn off initial guess for the first iteration\nop_tmp = LnL.inferOp(Xdata, zeros(100,1), zeros(100,1), Vr[:,1:2], Vr[:,1:2]' * Rtr[i], options)  # compute the first operator\nop[1] = op_tmp  # store the first operator\n\n# r =3\noptions.optim.initial_guess = true  # turn on initial guess for the next step\nop_tmp = LnL.inferOp(Xdata, zeros(100,1), zeros(100,1), Vr[:,1:3], Vr[:,1:3]' * Rtr[i], options, LnL.operators(A=op_tmp.A, F=op_tmp.F)) # compute the second operator\nop[2] = op_tmp","category":"page"},{"location":"manual/Options/#Linear-Solvers","page":"Options","title":"Linear Solvers","text":"","category":"section"},{"location":"manual/Options/","page":"Options","title":"Options","text":"For the optimization we use Ipopt, and for its linear solvers it is possible to use the default solvers like MUMPS and HSL solvers (e.g., ma77, ma86, and ma97).","category":"page"},{"location":"manual/Options/","page":"Options","title":"Options","text":"warning: Warning\nTo use HSL linear solvers you will need to obtain a license from here. Then set the path to the solvers using the option HSL_lib_path:import HSL_jll\nopt_settings.HSL_lib_path = HSL_jll.libhsl_path","category":"page"},{"location":"manual/Options/#Regularization","page":"Options","title":"Regularization","text":"","category":"section"},{"location":"manual/Options/","page":"Options","title":"Options","text":"Using the λtik struct you will define the Tikhonov regularization matrix. This will be a diagonal matrix with diagonal entries having different values corresponding to the operators that it is regulating (e.g., linear, quadratic, bilinear). ","category":"page"},{"location":"manual/Options/","page":"Options","title":"Options","text":"The Tikhonov regulated optimization problem is defined as ","category":"page"},{"location":"manual/Options/","page":"Options","title":"Options","text":"textminmathbfDmathbfO^top - dotmathbfhatX^2_F + mathbfGammamathbfO^top^2_F","category":"page"},{"location":"manual/Options/","page":"Options","title":"Options","text":"where mathbfD, mathbfO^top, dothatmathbfX are the data matrix, operator matrix (with minimizers), and time derivative snapshot matrix, respectively. The linear least-squares solution of this becomes","category":"page"},{"location":"manual/Options/","page":"Options","title":"Options","text":"mathbfO^top = (mathbfD^topmathbfD + mathbfGamma^topmathbfGamma)^dag mathbfDdotmathbfhatX","category":"page"},{"location":"manual/Options/","page":"Options","title":"Options","text":"λtik","category":"page"},{"location":"manual/Options/#LiftAndLearn.λtik","page":"Options","title":"LiftAndLearn.λtik","text":"struct λtik\n\nTikhonov regularization parameters.\n\nFields\n\nlin::Float64: the Tikhonov regularization parameter for linear state operator\nquad::Float64: the Tikhonov regularization parameter for quadratic state operator\nctrl::Float64: the Tikhonov regularization parameter for control operator\nbilin::Float64: the Tikhonov regularization parameter for bilinear state operator\n\n\n\n\n\n","category":"type"},{"location":"manual/Options/#Model-Reduction-Specific-Options","page":"Options","title":"Model Reduction Specific Options","text":"","category":"section"},{"location":"manual/Options/","page":"Options","title":"Options","text":"These options are all specific to each solution method of Operator Inference. All of the options below are a subtype of Abstract_Options.","category":"page"},{"location":"manual/Options/","page":"Options","title":"Options","text":"LiftAndLearn.Abstract_Options","category":"page"},{"location":"manual/Options/#LiftAndLearn.Abstract_Options","page":"Options","title":"LiftAndLearn.Abstract_Options","text":"Abstract_Options\n\nAbstract type for the options.\n\n\n\n\n\n","category":"type"},{"location":"manual/Options/#Standard-Operator-Inference","page":"Options","title":"Standard Operator Inference","text":"","category":"section"},{"location":"manual/Options/","page":"Options","title":"Options","text":"This option is required when using the standard Operator Inference method.","category":"page"},{"location":"manual/Options/","page":"Options","title":"Options","text":"LS_options","category":"page"},{"location":"manual/Options/#LiftAndLearn.LS_options","page":"Options","title":"LiftAndLearn.LS_options","text":"mutable struct LS_options <: LiftAndLearn.Abstract_Options\n\nStandard Operator Inference.\n\nFields\n\nmethod::String: the name of the method\nsystem::sys_struct: the system structure\nvars::vars: the system variables\ndata::data: the data\noptim::opt_settings: the optimization settings\nλ::λtik: the Tikhonov regularization parameters\nwith_tol::Bool: the option to use tolerance for the least square pseudo inverse\nwith_reg::Bool: the option to use Tikhonov regularization\npinv_tol::Real: the tolerance for the least square pseudo inverse\n\n\n\n\n\n","category":"type"},{"location":"manual/Options/","page":"Options","title":"Options","text":"note: Note\nThe option with_tol turns on/off the settings to truncate ill-posed singular values with order of pinv_tol.","category":"page"},{"location":"manual/Options/#Non-constrained-Operator-Inference","page":"Options","title":"Non-constrained Operator Inference","text":"","category":"section"},{"location":"manual/Options/","page":"Options","title":"Options","text":"This optimization is no different from the standard Operator Inference. The difference from the one above is that it is solved using a optimization package and not using simple linear algebra.","category":"page"},{"location":"manual/Options/","page":"Options","title":"Options","text":"NC_options","category":"page"},{"location":"manual/Options/#LiftAndLearn.NC_options","page":"Options","title":"LiftAndLearn.NC_options","text":"mutable struct NC_options <: LiftAndLearn.Abstract_Options\n\nNon-Constrained Operator Inference.\n\nFields\n\nmethod::String: the name of the method\nsystem::sys_struct: the system structure\nvars::vars: the system variables\ndata::data: the data\noptim::opt_settings: the optimization settings\nλ_lin::Real: the Tikhonov regularization parameter for linear state operator\nλ_quad::Real: the Tikhonov regularization parameter for quadratic state operator\n\n\n\n\n\n","category":"type"},{"location":"manual/Options/#Energy-Preserving-Operator-Inference-Options","page":"Options","title":"Energy-Preserving Operator Inference Options","text":"","category":"section"},{"location":"manual/Options/","page":"Options","title":"Options","text":"The three options below are for the energy-preserving Operator Inference approaches: hard equality constraint, soft inequality constraint, and penalty. For details of each parameter please check out the documentation of EP-OpInf Manual","category":"page"},{"location":"manual/Options/","page":"Options","title":"Options","text":"EPHEC_options\nEPSIC_options\nEPP_options","category":"page"},{"location":"manual/Options/#LiftAndLearn.EPHEC_options","page":"Options","title":"LiftAndLearn.EPHEC_options","text":"mutable struct EPHEC_options <: LiftAndLearn.Abstract_Options\n\nEnergy-Preserving Hard Equality Constraint Operator Inference.\n\nFields\n\nmethod::String: the name of the method\nsystem::sys_struct: the system structure\nvars::vars: the system variables\ndata::data: the data\noptim::opt_settings: the optimization settings\nλ_lin::Real: the Tikhonov regularization parameter for linear state operator\nλ_quad::Real: the Tikhonov regularization parameter for quadratic state operator\nA_bnds::Tuple{Float64, Float64}: the bounds for the linear operator\nForH_bnds::Tuple{Float64, Float64}: the bounds for the quadratic operator (F or H)\n\n\n\n\n\n","category":"type"},{"location":"manual/Options/#LiftAndLearn.EPSIC_options","page":"Options","title":"LiftAndLearn.EPSIC_options","text":"mutable struct EPSIC_options <: LiftAndLearn.Abstract_Options\n\nEnergy-Preserving Soft Inequality Constraint Operator Inference.\n\nFields\n\nmethod::String: the name of the method\nsystem::sys_struct: the system structure\nvars::vars: the system variables\ndata::data: the data\noptim::opt_settings: the optimization settings\nλ_lin::Real: the Tikhonov regularization parameter for linear state operator\nλ_quad::Real: the Tikhonov regularization parameter for quadratic state operator\nϵ::Real: soft constraint radius\nA_bnds::Tuple{Float64, Float64}: the bounds for the linear operator\nForH_bnds::Tuple{Float64, Float64}: the bounds for the quadratic operator (F or H)\n\n\n\n\n\n","category":"type"},{"location":"manual/Options/#LiftAndLearn.EPP_options","page":"Options","title":"LiftAndLearn.EPP_options","text":"mutable struct EPP_options <: LiftAndLearn.Abstract_Options\n\nEnergy-Preserving Penalty Operator Inference.\n\nFields\n\nmethod::String: the name of the method\nsystem::sys_struct: the system structure\nvars::vars: the system variables\ndata::data: the data\noptim::opt_settings: the optimization settings\nλ_lin::Real: the Tikhonov regularization parameter for linear state operator\nλ_quad::Real: the Tikhonov regularization parameter for quadratic state operator\nα::Float64: the weight for the energy-preserving term in the cost function\nA_bnds::Tuple{Float64, Float64}: the bounds for the linear operator\nForH_bnds::Tuple{Float64, Float64}: the bounds for the quadratic operator (F or H)\n\n\n\n\n\n","category":"type"},{"location":"manual/nonintrusive/EPOpInf/#Energy-Preserving-Operator-Inference","page":"Energy Preserving","title":"Energy-Preserving Operator Inference","text":"","category":"section"},{"location":"manual/nonintrusive/EPOpInf/","page":"Energy Preserving","title":"Energy Preserving","text":"This is a modified version of Operator Inference. Namely, it solves a constrained optimization for the reduced operators.","category":"page"},{"location":"manual/nonintrusive/EPOpInf/#Energy-Preserving-Quadratic-Nonlinearities-in-PDEs","page":"Energy Preserving","title":"Energy Preserving Quadratic Nonlinearities in PDEs","text":"","category":"section"},{"location":"manual/nonintrusive/EPOpInf/","page":"Energy Preserving","title":"Energy Preserving","text":"We consider an n-dimensional ordinary differential equation (ODE) which is linear and quadratic in the state mathbf x. Such an ODE often arises from spatially discretizing a PDE and is given by","category":"page"},{"location":"manual/nonintrusive/EPOpInf/","page":"Energy Preserving","title":"Energy Preserving","text":"    dotmathbf x(t) = mathbf A mathbf x(t) + mathbf H left(mathbf x(t) otimes mathbf x(t)right) ","category":"page"},{"location":"manual/nonintrusive/EPOpInf/","page":"Energy Preserving","title":"Energy Preserving","text":"where mathbf x(t) in mathbbR^n is the system state vector over t in 0 T_textfinal, and otimes denotes the Kronecker product. The operators mathbf A in mathbbR^ntimes n and mathbf H in mathbbR^ntimes n^2 are the linear and quadratic operators, respectively. In our setting, n is large, so simulating the system is computationally expensive.","category":"page"},{"location":"manual/nonintrusive/EPOpInf/","page":"Energy Preserving","title":"Energy Preserving","text":"The quadratic operator mathbf H is called `energy-preserving' if","category":"page"},{"location":"manual/nonintrusive/EPOpInf/","page":"Energy Preserving","title":"Energy Preserving","text":"    langle mathbf x mathbf H (mathbf x otimes mathbf x)rangle = mathbf x^top mathbf H(mathbf x otimes mathbf x) = 0 qquad text for all  mathbf x in mathbb R^n","category":"page"},{"location":"manual/nonintrusive/EPOpInf/","page":"Energy Preserving","title":"Energy Preserving","text":"This condition is derived by setting the quadratic term in the time derivative of the energy, frac12mathbfx^2, to zero.","category":"page"},{"location":"manual/nonintrusive/EPOpInf/#Energy-Preserving-Operator-Inference-(EP-OpInf)","page":"Energy Preserving","title":"Energy-Preserving Operator Inference (EP-OpInf)","text":"","category":"section"},{"location":"manual/nonintrusive/EPOpInf/","page":"Energy Preserving","title":"Energy Preserving","text":"To impose this energy-preserving structure on the operator, we propose EP-OpInf. For this method, we incorporate the constraint into the standard OpInf optimization and formulate a constrained minimization as follows:","category":"page"},{"location":"manual/nonintrusive/EPOpInf/","page":"Energy Preserving","title":"Energy Preserving","text":"    textbfEP-OpInf qquad \n    min_mathbfOinmathbb R^rtimes(r+r^2) mathbfDmathbfO^top - dothatmathbfX_F^2 quad text subject to  quad hat h_ijk + hat h_jik + hat h_kij = 0 quad  1 leq ijk leq r  ","category":"page"},{"location":"manual/nonintrusive/EPOpInf/","page":"Energy Preserving","title":"Energy Preserving","text":"note: Multiple Optimization Methods\nFor EP-OpInf, we implement multiple variations of the EP constraint.A hard equality constraint.\nA soft inequality constraint.\nA Penalty method where the constraint is indirectly applied to the problem.Note that the hard equality constraint is the enforces the constraint in the strictest manner and the penalty method is the weakest. The inequality constraint lies between the two. However, depending on the parameter settings the constraint violations may vary between the inequality and penalty methods.","category":"page"},{"location":"manual/nonintrusive/EPOpInf/","page":"Energy Preserving","title":"Energy Preserving","text":"EPHEC_Optimize\nEPSIC_Optimize\nEPP_Optimize","category":"page"},{"location":"manual/nonintrusive/EPOpInf/#LiftAndLearn.EPHEC_Optimize","page":"Energy Preserving","title":"LiftAndLearn.EPHEC_Optimize","text":"EPHEC_Optimize(D::Matrix, Rt::Union{Matrix,Transpose}, dims::Dict, \n    options::Abstract_Options, IG::operators) → Ahat, Bhat, Fhat, Hhat, Nhat, Khat\n\nEnergy preserved (Hard Equality Constraint) operator inference optimization (EPHEC)\n\nArguments\n\nD: data matrix\nRt: transpose of the derivative matrix\ndims: important dimensions\noptions: options for the operator inference set by the user\nIG: Initial Guesses\n\nReturns\n\nInferred operators\n\n\n\n\n\n","category":"function"},{"location":"manual/nonintrusive/EPOpInf/#LiftAndLearn.EPSIC_Optimize","page":"Energy Preserving","title":"LiftAndLearn.EPSIC_Optimize","text":"EPSIC_Optimize(D::Matrix, Rt::Union{Matrix,Transpose}, dims::Dict, \n    options::Abstract_Options, IG::operators) → Ahat, Bhat, Fhat, Hhat, Nhat, Khat\n\nEnergy preserved (Soft Inequality Constraint) operator inference optimization (EPSIC)\n\nArguments\n\nD: data matrix\nRt: transpose of the derivative matrix\ndims: important dimensions\noptions: options for the operator inference set by the user\nIG: Initial Guesses\n\nReturns\n\nInferred operators\n\n\n\n\n\n","category":"function"},{"location":"manual/nonintrusive/EPOpInf/#LiftAndLearn.EPP_Optimize","page":"Energy Preserving","title":"LiftAndLearn.EPP_Optimize","text":"EPP_Optimize(D::Matrix, Rt::Union{Matrix,Transpose}, dims::Dict, \n    options::Abstract_Options, IG::operators) → Ahat, Bhat, Fhat, Hhat, Nhat, Khat\n\nEnergy preserving penalty operator inference optimization (EPP)\n\nArguments\n\nD: data matrix\nRt: transpose of the derivative matrix\ndims: important dimensions\noptions: options for the operator inference set by the user\nIG: Initial Guesses\n\nReturns\n\nInferred operators\n\n\n\n\n\n","category":"function"},{"location":"manual/nonintrusive/LS/#Standard-Operator-Inference-(OpInf)","page":"Standard OpInf","title":"Standard Operator Inference (OpInf)","text":"","category":"section"},{"location":"manual/nonintrusive/LS/#Theory","page":"Standard OpInf","title":"Theory","text":"","category":"section"},{"location":"manual/nonintrusive/LS/","page":"Standard OpInf","title":"Standard OpInf","text":"If we consider a linear-quadratic system, the goal of Operator Inference is to non-intrusively obtain a reduced model of the form","category":"page"},{"location":"manual/nonintrusive/LS/","page":"Standard OpInf","title":"Standard OpInf","text":"    dothatmathbf x(t) = hatmathbf Ahatmathbf x(t) + hatmathbfH(hatmathbfx(t) otimes hatmathbfx(t))","category":"page"},{"location":"manual/nonintrusive/LS/","page":"Standard OpInf","title":"Standard OpInf","text":"To do so, we will fit reduced operators hatmathbfA and hatmathbfH to the reduced data in a least-squares sense. In addition to the state trajectory data ","category":"page"},{"location":"manual/nonintrusive/LS/","page":"Standard OpInf","title":"Standard OpInf","text":"    mathbfX = beginbmatrix \n               \n        boldsymbolmathbf x(t_1)  mathbfx(t_2)  cdots  mathbfx(t_K) \n               \n    endbmatrix in mathbbR^ntimes K ","category":"page"},{"location":"manual/nonintrusive/LS/","page":"Standard OpInf","title":"Standard OpInf","text":"we also require paired state time derivative data: ","category":"page"},{"location":"manual/nonintrusive/LS/","page":"Standard OpInf","title":"Standard OpInf","text":"beginaligned\n    dotmathbfX = beginbmatrix\n              \n        dotmathbfx(t_1)  dotmathbfx(t_2)  cdots  dotmathbfx(t_K) \n              \n    endbmatrixinmathbbR^ntimes K quad text where  \n    dotmathbf x(t_i)=mathbf Amathbf x(t_i)+ mathbf H (mathbf x(t_i) otimes mathbf x(t_i))\nendaligned","category":"page"},{"location":"manual/nonintrusive/LS/","page":"Standard OpInf","title":"Standard OpInf","text":"This time derivative data can come directly from the simulation of the high-dimensional ODE or can be approximated numerically from the snapshot data. We use the POD basis mathbfV_r to compute reduced state and time derivative data as follows: let hatmathbfx_i = mathbfV_r^topmathbfx(t_i) as an ansatz, and dothatmathbfx_i = mathbfV_r^topdotmathbfx(t_i) for i=1ldotsK. Then, define","category":"page"},{"location":"manual/nonintrusive/LS/","page":"Standard OpInf","title":"Standard OpInf","text":"    hattextbf X = beginbmatrix\n               \n        boldsymbolhatmathbf x_1  boldsymbol hatmathbf x_2  cdots  boldsymbolhatmathbf x_K \n               \n    endbmatrix in mathbb R^rtimes K qquad textand qquad \n    dothattextbf X = beginbmatrix\n               \n        dothatmathbf x_1  dothatmathbf x_2  cdots  dothatmathbf x_K \n               \n    endbmatrix in mathbb R^rtimes K","category":"page"},{"location":"manual/nonintrusive/LS/","page":"Standard OpInf","title":"Standard OpInf","text":"Additionally, we define the matrix formed by the quadratic terms of the state data","category":"page"},{"location":"manual/nonintrusive/LS/","page":"Standard OpInf","title":"Standard OpInf","text":"    hattextbf X_otimes = beginbmatrix\n               \n        (hatmathbf x_1 otimes hatmathbf x_1)  (hatmathbf x_2 otimes hatmathbf x_2)  cdots  (hatmathbf x_K otimes hatmathbf x_K) \n               \n    endbmatrix in mathbb R^r^2times K","category":"page"},{"location":"manual/nonintrusive/LS/","page":"Standard OpInf","title":"Standard OpInf","text":"This allows us to formulate the following minimization for finding the reduced operators hatmathbfA and hatmathbfH:   ","category":"page"},{"location":"manual/nonintrusive/LS/","page":"Standard OpInf","title":"Standard OpInf","text":"    textbfStandard OpInf qquad \n    min_mathbfhatAinmathbb R^rtimes rhatmathbfHinmathbb R^rtimes r^2 sum_i=1^K left dothatmathbf x_i - mathbfhatAhatmathbfx_i - hatmathbf H(hatmathbf x_i otimes hatmathbf x_i) right ^2_2 = min_mathbfOinmathbb R^rtimes(r+r^2) mathbfDmathbfO^top - dothatmathbf X_F^2","category":"page"},{"location":"manual/nonintrusive/LS/","page":"Standard OpInf","title":"Standard OpInf","text":"where mathbf D = hatmathbf X^top hatmathbf X_otimes^top in mathbb R^Ktimes(r+r^2) and mathbf O = mathbfhat A hatmathbf H in mathbb R^rtimes(r+r^2).","category":"page"},{"location":"manual/nonintrusive/LS/#Implementation","page":"Standard OpInf","title":"Implementation","text":"","category":"section"},{"location":"manual/nonintrusive/LS/","page":"Standard OpInf","title":"Standard OpInf","text":"In this package we implement the standard OpInf along with Tikhonov regularized version with the function inferOp.","category":"page"},{"location":"manual/nonintrusive/LS/","page":"Standard OpInf","title":"Standard OpInf","text":"There are many things going on under the hood when the function: ","category":"page"},{"location":"manual/nonintrusive/LS/","page":"Standard OpInf","title":"Standard OpInf","text":"constructing the data matrix\nconstruting the Tikhonov matrix\nreprojection","category":"page"},{"location":"manual/nonintrusive/LS/","page":"Standard OpInf","title":"Standard OpInf","text":"But all of those operations are taken care of automatically. For full details please see the source code.","category":"page"},{"location":"manual/nonintrusive/LS/","page":"Standard OpInf","title":"Standard OpInf","text":"inferOp","category":"page"},{"location":"manual/nonintrusive/LS/#LiftAndLearn.inferOp","page":"Standard OpInf","title":"LiftAndLearn.inferOp","text":"inferOp(X::Matrix, U::Matrix, Y::VecOrMat, Vn::Matrix, R::Matrix,\n    options::Abstract_Options, IG::operators=operators()) → op::operators\n\nInfer the operators with derivative data given\n\nArguments\n\nX::Matrix: state data matrix\nU::Matrix: input data matrix\nY::VecOrMat: output data matix\nVn::Matrix: POD basis\nR::Matrix: derivative data matrix\noptions::Abstract_Options: options for the operator inference defined by the user\nIG::operators: initial guesses for optimization\n\nReturns\n\nop::operators: inferred operators\n\n\n\n\n\ninferOp(X::Matrix, U::Matrix, Y::VecOrMat, Vn::Matrix,\n    options::Abstract_Options, IG::operators=operators()) → op::operators\n\nInfer the operators without derivative data (dispatch)\n\nArguments\n\nX::Matrix: state data matrix\nU::Matrix: input data matrix\nY::VecOrMat: output data matix\nVn::Matrix: POD basis\noptions::Abstract_Options: options for the operator inference defined by the user\nIG::operators: initial guesses for optimization\n\nReturns\n\nop::operators: inferred operators\n\n\n\n\n\ninferOp(X::Matrix, U::Matrix, Y::VecOrMat, Vn::Matrix,\n    full_op::operators, options::Abstract_Options, IG::operators=operators()) → op::operators\n\nInfer the operators with reprojection method (dispatch)\n\nArguments\n\nX::Matrix: state data matrix\nU::Matrix: input data matrix\nY::VecOrMat: output data matix\nVn::Matrix: POD basis\nfull_op::operators: full order model operators\noptions::Abstract_Options: options for the operator inference defined by the user\nIG::operators: initial guesses for optimization\n\nReturns\n\nop::operators: inferred operators\n\n\n\n\n\ninferOp(W::Matrix, U::Matrix, Y::VecOrMat, Vn::Union{Matrix,BlockDiagonal},\n    lm::lifting, full_op::operators, options::Abstract_Options, \n    IG::operators=operators()) → op::operators\n\nInfer the operators for Lift And Learn for reprojected data (dispatch)\n\nArguments\n\nW::Matrix: state data matrix\nU::Matrix: input data matrix\nY::VecOrMat: output data matix\nVn::Union{Matrix,BlockDiagonal}: POD basis\nlm::lifting: struct of the lift map\nfull_op::operators: full order model operators\noptions::Abstract_Options: options for the operator inference defined by the user\nIG::operators: initial guesses for optimization\n\nReturns\n\nop::operators: inferred operators\n\n\n\n\n\n","category":"function"},{"location":"manual/nonintrusive/LS/#Optimization-Implementation","page":"Standard OpInf","title":"Optimization Implementation","text":"","category":"section"},{"location":"manual/nonintrusive/LS/","page":"Standard OpInf","title":"Standard OpInf","text":"There is a function that solves the least squares problem using Ipopt as well.","category":"page"},{"location":"manual/nonintrusive/LS/","page":"Standard OpInf","title":"Standard OpInf","text":"NC_Optimize\nNC_Optimize_output","category":"page"},{"location":"manual/nonintrusive/LS/#LiftAndLearn.NC_Optimize","page":"Standard OpInf","title":"LiftAndLearn.NC_Optimize","text":"NC_Optimize(D::Matrix, Rt::Union{Matrix, Transpose}, dims::Dict, \n    options::Abstract_Options, IG::operators) → Ahat, Bhat, Fhat, Hhat, Nhat, Khat\n\nOptimization version of Standard Operator Inference (NC)\n\nArguments\n\nD: data matrix\nRt: transpose of the derivative matrix\ndims: important dimensions\noptions: options for the operator inference set by the user\nIG: Initial Guesses\n\nReturns\n\nInferred operators\n\n\n\n\n\n","category":"function"},{"location":"manual/nonintrusive/LS/#LiftAndLearn.NC_Optimize_output","page":"Standard OpInf","title":"LiftAndLearn.NC_Optimize_output","text":"NC_Optimize_output(Y::Matrix, Xt_hat::Union{Matrix, Transpose}, dims::Dict, options::Abstract_Options) → C\n\nOutput optimization for the standard operator inference (for operator C)\n\nArguments\n\nY: the output matrix \nXt_hat: the state matrix\n\nReturn\n\nthe output state matrix C\n\n\n\n\n\n","category":"function"},{"location":"models/Burgers/#Viscous-Burgers'-Equation","page":"Burgers","title":"Viscous Burgers' Equation","text":"","category":"section"},{"location":"models/Burgers/","page":"Burgers","title":"Burgers","text":"Modules = [LiftAndLearn.Burgers]","category":"page"},{"location":"models/Burgers/#LiftAndLearn.Burgers","page":"Burgers","title":"LiftAndLearn.Burgers","text":"Viscous Burgers' equation model\n\n\n\n\n\n","category":"module"},{"location":"models/Burgers/#LiftAndLearn.Burgers.Abstract_Models","page":"Burgers","title":"LiftAndLearn.Burgers.Abstract_Models","text":"Abstract_Models\n\nAbstract type for the models.\n\n\n\n\n\n","category":"type"},{"location":"models/Burgers/#LiftAndLearn.Burgers.burgers","page":"Burgers","title":"LiftAndLearn.Burgers.burgers","text":"mutable struct burgers <: LiftAndLearn.Burgers.Abstract_Models\n\nViscous Burgers' equation model\n\nfracpartial upartial t = mufracpartial^2 upartial x^2 - ufracpartial upartial x\n\nFields\n\nOmega::Vector{Float64}: spatial domain\nT::Vector{Float64}: temporal domain\nD::Vector{Float64}: parameter domain\nΔx::Float64: spatial grid size\nΔt::Float64: temporal step size\nIC::VecOrMat{Float64}: initial condition\nx::Vector{Float64}: spatial grid points\nt::Vector{Float64}: temporal points\nμs::Union{Vector{Float64},Float64}: parameter vector\nXdim::Int64: spatial dimension\nTdim::Int64: temporal dimension\nPdim::Int64: parameter dimension\nBC::String: boundary condition\ngenerateABFmatrix::Function: function to generate A, B, F matrices\ngenerateMatrix_NC_periodic::Function: function to generate A, F matrices for the non-energy preserving Burgers' equation. (Non-conservative Periodic boundary condition)\ngenerateMatrix_C_periodic::Function: function to generate A, F matrices for the non-energy preserving Burgers' equation. (conservative periodic boundary condition)\ngenerateEPmatrix::Function: function to generate A, F matrices for the Burgers' equation. (Energy-preserving form)\nsemiImplicitEuler::Function: function to integrate the system using semi-implicit Euler scheme\n\n\n\n\n\n","category":"type"},{"location":"models/Burgers/#LiftAndLearn.Burgers.burgers-NTuple{7, Any}","page":"Burgers","title":"LiftAndLearn.Burgers.burgers","text":"burgers(Omega, T, D, Δx, Δt, Pdim, BC) → burgers\n\nConstructor for the Burgers' equation model.\n\n\n\n\n\n","category":"method"},{"location":"models/Burgers/#LiftAndLearn.Burgers.generateABFmatrix-Tuple{LiftAndLearn.Burgers.burgers, Float64}","page":"Burgers","title":"LiftAndLearn.Burgers.generateABFmatrix","text":"generateABFmatrix(model, μ) → A, B, F\n\nGenerate A, B, F matrices for the Burgers' equation.\n\nArguments\n\nmodel: Burgers' equation model\nμ: parameter value\n\nReturns\n\nA: A matrix\nB: B matrix\nF: F matrix\n\n\n\n\n\n","category":"method"},{"location":"models/Burgers/#LiftAndLearn.Burgers.generateEPmatrix-Tuple{LiftAndLearn.Burgers.burgers, Float64}","page":"Burgers","title":"LiftAndLearn.Burgers.generateEPmatrix","text":"generateEPmatrix(model, μ) → A, F\n\nGenerate A, F matrices for the Burgers' equation. (Energy-preserving form)\n\nArguments\n\nmodel: Burgers' equation model\nμ: parameter value\n\nReturns\n\nA: A matrix\nF: F matrix\n\n\n\n\n\n","category":"method"},{"location":"models/Burgers/#LiftAndLearn.Burgers.generateMatrix_C_periodic-Tuple{LiftAndLearn.Burgers.burgers, Float64}","page":"Burgers","title":"LiftAndLearn.Burgers.generateMatrix_C_periodic","text":"generateMatrix_C_periodic(model, μ) → A, F\n\nGenerate A, F matrices for the non-energy preserving Burgers' equation. (conservative periodic boundary condition)\n\nArguments\n\nmodel: Burgers' equation model\nμ: parameter value\n\nReturns\n\nA: A matrix\nF: F matrix\n\n\n\n\n\n","category":"method"},{"location":"models/Burgers/#LiftAndLearn.Burgers.generateMatrix_NC_periodic-Tuple{LiftAndLearn.Burgers.burgers, Float64}","page":"Burgers","title":"LiftAndLearn.Burgers.generateMatrix_NC_periodic","text":"generateMatrix_NC_periodic(model, μ) → A, F\n\nGenerate A, F matrices for the non-energy preserving Burgers' equation. (Non-conservative Periodic boundary condition)\n\nArguments\n\nmodel: Burgers' equation model\nμ: parameter value\n\nReturns\n\nA: A matrix\nF: F matrix\n\n\n\n\n\n","category":"method"},{"location":"models/Burgers/#LiftAndLearn.Burgers.semiImplicitEuler-NTuple{4, Any}","page":"Burgers","title":"LiftAndLearn.Burgers.semiImplicitEuler","text":"semiImplicitEuler(A, F, tdata, IC) → states\n\nSemi-Implicit Euler scheme without control (dispatch)\n\nArguments\n\nA: linear state operator\nF: quadratic state operator\ntdata: time data\nIC: initial condtions\n\nReturns\n\nstates: integrated states\n\n\n\n\n\n","category":"method"},{"location":"models/Burgers/#LiftAndLearn.Burgers.semiImplicitEuler-NTuple{6, Any}","page":"Burgers","title":"LiftAndLearn.Burgers.semiImplicitEuler","text":"semiImplicitEuler(A, B, F, U, tdata, IC) → states\n\nSemi-Implicit Euler scheme\n\nArguments\n\nA: linear state operator\nB: linear input operator\nF: quadratic state operator\nU: input data\ntdata: time data\nIC: initial condtions\n\nReturns\n\nstates: integrated states\n\n\n\n\n\n","category":"method"},{"location":"models/Burgers/#LiftAndLearn.Burgers.vech-Union{Tuple{AbstractMatrix{T}}, Tuple{T}} where T","page":"Burgers","title":"LiftAndLearn.Burgers.vech","text":"vech(A) → v\n\nHalf-vectorization operation\n\nArguments\n\nA: matrix to half-vectorize\n\nReturns\n\nv: half-vectorized form\n\n\n\n\n\n","category":"method"},{"location":"paper/#Paper","page":"Paper Reference","title":"Paper","text":"","category":"section"},{"location":"paper/","page":"Paper Reference","title":"Paper Reference","text":"Below you have a list of publications referenced in this work.","category":"page"},{"location":"paper/","page":"Paper Reference","title":"Paper Reference","text":"W. I. Uy, Y. Wang, Y. Wen and B. Peherstorfer. Active Operator Inference for Learning Low-Dimensional Dynamical-System Models from Noisy Data. SIAM Journal on Scientific Computing 45, A1462–A1490 (2023).\n\n\n\nB. Peherstorfer. Sampling Low-Dimensional Markovian Dynamics for Preasymptotically Recovering Reduced Models from Data with Operator Inference. SIAM Journal on Scientific Computing 42, A3489–A3515 (2020).\n\n\n\nJ. L. Lumley. The structure of inhomogeneous turbulent flows. Atmospheric turbulence and radio wave propagation, 166–178 (1967).\n\n\n\nL. Sirovich. Turbulence and the dynamics of coherent structures. I. Coherent structures. Quarterly of applied mathematics 45, 561–571 (1987).\n\n\n\nG. Berkooz, P. Holmes and J. L. Lumley. The proper orthogonal decomposition in the analysis of turbulent flows. Annual review of fluid mechanics 25, 539–575 (1993).\n\n\n\n","category":"page"},{"location":"manual/nonintrusive/LnL/#Lift-And-Learn","page":"Lift And Learn","title":"Lift And Learn","text":"","category":"section"},{"location":"manual/nonintrusive/LnL/#Overview","page":"Lift And Learn","title":"Overview","text":"","category":"section"},{"location":"manual/nonintrusive/LnL/","page":"Lift And Learn","title":"Lift And Learn","text":"The Lift and Learn method offers a transformative approach to model reduction in nonlinear dynamical systems, especially those not conforming to polynomial structures. ","category":"page"},{"location":"manual/nonintrusive/LnL/#Lifting-Process","page":"Lift And Learn","title":"Lifting Process","text":"","category":"section"},{"location":"manual/nonintrusive/LnL/","page":"Lift And Learn","title":"Lift And Learn","text":"This method introduces an auxiliary variable, effectively 'lifting' the system into a polynomial framework. This direct transformation contrasts with Koopman theory's approximation approach, streamlining the representation of complex dynamics.","category":"page"},{"location":"manual/nonintrusive/LnL/#Operator-Inference","page":"Lift And Learn","title":"Operator Inference","text":"","category":"section"},{"location":"manual/nonintrusive/LnL/","page":"Lift And Learn","title":"Lift And Learn","text":"After lifting, the system is compatible with the Operator Inference scheme, facilitating the discovery of simplified models through data-driven techniques. This phase is instrumental in deriving interpretable and efficient reduced-order models from intricate dynamical systems.","category":"page"},{"location":"manual/nonintrusive/LnL/","page":"Lift And Learn","title":"Lift And Learn","text":"For further details on lifting, please see lifting.","category":"page"},{"location":"manual/Lift/#Lifting","page":"Lift","title":"Lifting","text":"","category":"section"},{"location":"manual/Lift/","page":"Lift","title":"Lift","text":"This page gives an explanation for the mathematical concept of lifting and provides example code for its implementation.","category":"page"},{"location":"manual/Lift/#Lift-Map-Structure","page":"Lift","title":"Lift Map Structure","text":"","category":"section"},{"location":"manual/Lift/","page":"Lift","title":"Lift","text":"lifting","category":"page"},{"location":"manual/Lift/#LiftAndLearn.lifting","page":"Lift","title":"LiftAndLearn.lifting","text":"struct lifting\n\nLifting map structure.\n\nFields\n\nN: number of variables of the original nonlinear dynamics\nNl: number of variables of the lifted system\nlift_funcs: array of lifting transformation functions \nmap: function to map the data to the new mapped states including original states\nmapNL: function to map the data to only the additional lifted states \n\n\n\n\n\n","category":"type"},{"location":"manual/Lift/","page":"Lift","title":"Lift","text":"tip: Tip\nFor a simple pendulum we have beginbmatrix\ndotx_1 \ndotx_2 \nendbmatrix = beginbmatrix\nx_2 \n-fracgl sin(x_1)\nendbmatrixThe lifted system becomes beginbmatrix\ndotx_1 \ndotx_2 \ndotx_3 \ndotx_4\nendbmatrix = beginbmatrix\nx_2 \n-fracgl x_3 \nx_2 x_4 \n-x_2 x_3\nendbmatrixwhen x_3 = sin(x_1) and x_4 = cos(x_1). Which if coded, would look like this:lifter = LnL.lifting(2, 4, [x -> sin.(x[1]), x -> cos.(x[1])])","category":"page"},{"location":"manual/Lift/#Construct-Lifted-POD-Basis-from-Data","page":"Lift","title":"Construct Lifted POD Basis from Data","text":"","category":"section"},{"location":"manual/Lift/","page":"Lift","title":"Lift","text":"liftedBasis","category":"page"},{"location":"manual/Lift/#LiftAndLearn.liftedBasis","page":"Lift","title":"LiftAndLearn.liftedBasis","text":"liftedBasis(W, Nl, gp, ro) → Vr\n\nCreate the block-diagonal POD basis for the new lifted system data\n\nArguments\n\nw: lifted data matrix\nNl: number of variables of the lifted state dynamics\ngp: number of grid points for each variable\nro: vector of the reduced orders for each basis\n\nReturn\n\nVr: block diagonal POD basis\n\n\n\n\n\n","category":"function"},{"location":"manual/Lift/","page":"Lift","title":"Lift","text":"An example implementation would be:","category":"page"},{"location":"manual/Lift/","page":"Lift","title":"Lift","text":"using LiftAndLearn\nusing Random\nLnL = LiftAndLearn\nW = round.(rand(30,100), digits=4)\nLnL.liftedBasis(W, 3, 10, [2,3,4])","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"CurrentModule = LiftAndLearn ","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Pages = [\"api.md\"]","category":"page"},{"location":"api/#API","page":"API Reference","title":"API","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"All APIs of LiftAndLearn listed in a unstructured manner.","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Modules = [LiftAndLearn]\nOrder = [:module, :function, :macro]","category":"page"},{"location":"api/#LiftAndLearn.EPConstraintResidual","page":"API Reference","title":"LiftAndLearn.EPConstraintResidual","text":"EPConstraintResidual(X, r, which_quad=\"H\"; with_mmt=false) → ϵX, mmt\n\nCompute the constraint residual which is the residual of the energy-preserving constraint \n\nsum left hath_ijk + hath_jik + hath_kji right quad 1 leq ijk leq r\n\nArguments\n\nX::Union{Matrix,SparseMatrixCSC}: the matrix to compute the constraint residual\nr::Real: the dimension of the system\nwhich_quad::String: the type of the quadratic operator (H or F)\nwith_mmt::Bool: whether to compute the moment of the constraint residual\n\nReturns\n\nϵX: the constraint residual\nmmt: the moment which is the sum of the constraint residual without absolute value\n\n\n\n\n\n","category":"function"},{"location":"api/#LiftAndLearn.EPConstraintViolation","page":"API Reference","title":"LiftAndLearn.EPConstraintViolation","text":"EPConstraintViolation(Data, X, which_quad=\"H\") → viol\n\nCompute the constraint violation which is the violation of the energy-preserving constraint\n\nsum langle mathbfx mathbfH(mathbfxotimesmathbfx)rangle quad forall mathbfx in mathcalD\n\nArguments\n\nData::AbstractArray: the data\nX::Union{Matrix,SparseMatrixCSC}: the matrix to compute the constraint violation\nwhich_quad::String: the type of the quadratic operator (H or F)\n\nReturns\n\nviol: the constraint violation\n\n\n\n\n\n","category":"function"},{"location":"api/#LiftAndLearn.EPHEC_Optimize-Tuple{Matrix{T} where T, Union{Matrix{T} where T, LinearAlgebra.Transpose}, Dict, LiftAndLearn.Abstract_Options, operators}","page":"API Reference","title":"LiftAndLearn.EPHEC_Optimize","text":"EPHEC_Optimize(D::Matrix, Rt::Union{Matrix,Transpose}, dims::Dict, \n    options::Abstract_Options, IG::operators) → Ahat, Bhat, Fhat, Hhat, Nhat, Khat\n\nEnergy preserved (Hard Equality Constraint) operator inference optimization (EPHEC)\n\nArguments\n\nD: data matrix\nRt: transpose of the derivative matrix\ndims: important dimensions\noptions: options for the operator inference set by the user\nIG: Initial Guesses\n\nReturns\n\nInferred operators\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.EPP_Optimize-Tuple{Matrix{T} where T, Union{Matrix{T} where T, LinearAlgebra.Transpose}, Dict, LiftAndLearn.Abstract_Options, operators}","page":"API Reference","title":"LiftAndLearn.EPP_Optimize","text":"EPP_Optimize(D::Matrix, Rt::Union{Matrix,Transpose}, dims::Dict, \n    options::Abstract_Options, IG::operators) → Ahat, Bhat, Fhat, Hhat, Nhat, Khat\n\nEnergy preserving penalty operator inference optimization (EPP)\n\nArguments\n\nD: data matrix\nRt: transpose of the derivative matrix\ndims: important dimensions\noptions: options for the operator inference set by the user\nIG: Initial Guesses\n\nReturns\n\nInferred operators\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.EPSIC_Optimize-Tuple{Matrix{T} where T, Union{Matrix{T} where T, LinearAlgebra.Transpose}, Dict, LiftAndLearn.Abstract_Options, operators}","page":"API Reference","title":"LiftAndLearn.EPSIC_Optimize","text":"EPSIC_Optimize(D::Matrix, Rt::Union{Matrix,Transpose}, dims::Dict, \n    options::Abstract_Options, IG::operators) → Ahat, Bhat, Fhat, Hhat, Nhat, Khat\n\nEnergy preserved (Soft Inequality Constraint) operator inference optimization (EPSIC)\n\nArguments\n\nD: data matrix\nRt: transpose of the derivative matrix\ndims: important dimensions\noptions: options for the operator inference set by the user\nIG: Initial Guesses\n\nReturns\n\nInferred operators\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.F2H-Tuple{Any}","page":"API Reference","title":"LiftAndLearn.F2H","text":"F2H(F::Union{SparseMatrixCSC,VecOrMat}) → H\n\nConvert the quadratic F operator into the H operator\n\nArguments\n\nF: F matrix\n\nReturns\n\nH: H matrix\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.F2Hs-Tuple{Any}","page":"API Reference","title":"LiftAndLearn.F2Hs","text":"F2Hs(F::Union{SparseMatrixCSC,VecOrMat}) → Hs\n\nConvert the quadratic F operator into the symmetric H operator.\n\nThis guarantees that the H operator is symmetric. The difference from F2H is that we use the elimination matrix L and the symmetric commutation matrix N to multiply the F matrix.\n\nArguments\n\nF: F matrix\n\nReturns\n\nHs: symmetric H matrix\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.H2F-Tuple{Any}","page":"API Reference","title":"LiftAndLearn.H2F","text":"H2F(H::Union{SparseMatrixCSC,VecOrMat}) → F\n\nConvert the quadratic H operator into the F operator\n\nArguments\n\nH: H matrix\n\nReturns\n\nF: F matrix\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.H2Q-Tuple{AbstractArray}","page":"API Reference","title":"LiftAndLearn.H2Q","text":"H2Q(H::AbstractArray) → Q\n\nConvert the quadratic H operator into the Q operator\n\nArguments\n\nH::AbstractArray: Quadratic matrix of dimensions (n x n^2)\n\nReturns\n\nthe Q quadratic matrix of 3-dim tensor\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.LS_solve-Tuple{Matrix{T} where T, Union{Matrix{T} where T, LinearAlgebra.Transpose}, Matrix{T} where T, Union{Matrix{T} where T, LinearAlgebra.Transpose}, Dict, LiftAndLearn.Abstract_Options}","page":"API Reference","title":"LiftAndLearn.LS_solve","text":"LS_solve(D::Matrix, Rt::Union{Matrix,Transpose}, Y::Matrix, Xhat_t::Union{Matrix,Transpose}, \n    dims::Dict, options::Abstract_Options) → Ahat, Bhat, Chat, Fhat, Hhat, Nhat, Khat\n\nSolve the standard Operator Inference with/without regularization\n\nArguments\n\nD::Matrix: data matrix\nRt::Union{Matrix,Transpose}: derivative data matrix (transposed)\nY::Matrix: output data matrix\nXhat_t::Union{Matrix,Transpose}: projected data matrix (transposed)\ndims::Dict: dictionary including important dimensions\noptions::Abstract_Options: options for the operator inference set by the user\n\nReturns\n\nAll learned operators A, B, C, F, H, N, K\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.NC_Optimize-Tuple{Matrix{T} where T, Union{Matrix{T} where T, LinearAlgebra.Transpose}, Dict, LiftAndLearn.Abstract_Options, operators}","page":"API Reference","title":"LiftAndLearn.NC_Optimize","text":"NC_Optimize(D::Matrix, Rt::Union{Matrix, Transpose}, dims::Dict, \n    options::Abstract_Options, IG::operators) → Ahat, Bhat, Fhat, Hhat, Nhat, Khat\n\nOptimization version of Standard Operator Inference (NC)\n\nArguments\n\nD: data matrix\nRt: transpose of the derivative matrix\ndims: important dimensions\noptions: options for the operator inference set by the user\nIG: Initial Guesses\n\nReturns\n\nInferred operators\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.NC_Optimize_output-Tuple{Matrix{T} where T, Union{Matrix{T} where T, LinearAlgebra.Transpose}, Dict, LiftAndLearn.Abstract_Options}","page":"API Reference","title":"LiftAndLearn.NC_Optimize_output","text":"NC_Optimize_output(Y::Matrix, Xt_hat::Union{Matrix, Transpose}, dims::Dict, options::Abstract_Options) → C\n\nOutput optimization for the standard operator inference (for operator C)\n\nArguments\n\nY: the output matrix \nXt_hat: the state matrix\n\nReturn\n\nthe output state matrix C\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.Q2H-Tuple{AbstractArray}","page":"API Reference","title":"LiftAndLearn.Q2H","text":"Q2H(Q::AbstractArray) → H\n\nConvert the quadratic Q operator into the H operator. The Q matrix is  a 3-dim tensor with dimensions (n x n x n). Thus,\n\nmathbfQ = beginbmatrix \n    mathbfQ_1  \n    mathbfQ_2  \n    vdots  \n    mathbfQ_n \nendbmatrix\nquad textwhere  mathbfQ_i in mathbbR^n times n\n\nArguments\n\nQ::AbstractArray: Quadratic matrix in the 3-dim tensor form with dimensions (n x n x n)\n\nReturns\n\nthe H quadratic matrix\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.backwardEuler-NTuple{5, Any}","page":"API Reference","title":"LiftAndLearn.backwardEuler","text":"backwardEuler(A, B, U, tdata, IC)\n\n\nBackward Euler scheme integration.\n\nArguments\n\nA: linear state operator\nB: linear input operator\nU: input data\ntdata: time data\nIC: initial condtions\n\nReturn\n\nstates: integrated states\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.choose_ro-Tuple{Vector{T} where T}","page":"API Reference","title":"LiftAndLearn.choose_ro","text":"choose_ro(Σ::Vector; en_low=-15) → r_all, en\n\nChoose reduced order (ro) that preserves an acceptable energy.\n\nArguments\n\nΣ::Vector: Singular value vector from the SVD of some Hankel Matrix\nen_low: minimum size for energy preservation\n\nReturns\n\nr_all: vector of reduced orders\nen: vector of energy values\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.commat-Tuple{Integer, Integer}","page":"API Reference","title":"LiftAndLearn.commat","text":"commat(m::Integer, n::Integer) → K\n\nCreate commutation matrix K of dimension m x n [magnus1980].\n\nArguments\n\nm::Integer: row dimension of the commutation matrix\nn::Integer: column dimension of the commutation matrix\n\nReturns\n\nK: commutation matrix\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.commat-Tuple{Integer}","page":"API Reference","title":"LiftAndLearn.commat","text":"commat(m::Integer) → K\n\nDispatch for the commutation matrix of dimensions (m, m)\n\nArguments\n\nm::Integer: row and column dimension of the commutation matrix\n\nReturns\n\nK: commutation matrix\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.compError-NTuple{7, Any}","page":"API Reference","title":"LiftAndLearn.compError","text":"compError(Xf, Yf, Xint, Yint, Xinf, Yinf, Vr) → PE, ISE, IOE, OSE, OOE\n\nCompute all projection, state, and output errors\n\nArguments\n\nXf: reference state data\nYf: reference output data\nXint: intrusive model state data\nYint: intrusive model output data\nXinf: inferred model state data\nXint: inferrred model output data\nVr: POD basis\n\nReturn\n\nPE: projection error\nISE: intrusive state error\nIOE: intrusive output error\nOSE: operator inference state error\nOOE: operator inference output error\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.compOutputError-Tuple{Any, Any}","page":"API Reference","title":"LiftAndLearn.compOutputError","text":"compOutputError(Yf, Y) → OE\n\nCompute output error\n\nArguments\n\nYf: reference output data\nY: testing output data\n\nReturn\n\nOE: output error\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.compProjError-Tuple{Any, Any}","page":"API Reference","title":"LiftAndLearn.compProjError","text":"compProjError(Xf, Vr) → PE\n\nCompute the projection error\n\nArguments\n\nXf: reference state data\nVr: POD basis\n\nReturn\n\nPE: projection error\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.compStateError-Tuple{Any, Any, Any}","page":"API Reference","title":"LiftAndLearn.compStateError","text":"compStateError(Xf, X, Vr) → SE\n\nCompute the state error\n\nArguments\n\nXf: reference state data\nX: testing state data\nVr: POD basis\n\nReturn\n\nSE: state error\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.crankNicolson-NTuple{5, Any}","page":"API Reference","title":"LiftAndLearn.crankNicolson","text":"crankNicolson(A, B, U, tdata, IC)\n\n\nCrank-Nicolson scheme\n\nArguments\n\nA: linear state operator\nB: linear input operator\nU: input data\ntdata: time data\nIC: initial condtions\n\nReturn\n\nstates: integrated states\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.delta-Tuple{Any, Any}","page":"API Reference","title":"LiftAndLearn.delta","text":"delta(v::Int, w::Int) → Float64\n\nAnother auxiliary function for the F matrix\n\nArguments\n\nv: first index\nw: second index\n\nReturns\n\ncoefficient of 1.0 or 0.5\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.dtApprox-Tuple{VecOrMat{T} where T, LiftAndLearn.Abstract_Options}","page":"API Reference","title":"LiftAndLearn.dtApprox","text":"dtApprox(X::VecOrMat, options::Abstract_Options) → dXdt, idx\n\nApproximating the derivative values of the data with different integration schemes\n\nArguments\n\nX::VecOrMat: data matrix\noptions::Abstract_Options: operator inference options\n\nReturns\n\ndXdt: derivative data\nidx: index for the specific integration scheme (important for later use)\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.dupmat-Tuple{Any}","page":"API Reference","title":"LiftAndLearn.dupmat","text":"dupmat(n::Integer) → D\n\nCreate duplication matrix D of dimension n [magnus1980].\n\nArguments\n\nn: dimension of the duplication matrix\n\nReturns\n\nD: duplication matrix\n\nExamples\n\njulia> A = [1 2; 3 4]\n2×2 Matrix{Int64}:\n 1  3\n 3  4\n\njulia> D = LnL.dupmat(2)\n4×3 SparseArrays.SparseMatrixCSC{Float64, Int64} with 4 stored entries:\n 1.0   ⋅    ⋅ \n  ⋅   1.0   ⋅\n  ⋅   1.0   ⋅\n  ⋅    ⋅   1.0\n\njulia> D * LnL.vech(A)\n4-element Vector{Float64}:\n 1.0\n 3.0\n 3.0\n 4.0\n\njulia> a = vec(A)\n4-element Vector{Int64}:\n 1\n 3\n 3\n 4\n\nReferences\n\n[magnus1980] J. R. Magnus and H. Neudecker, “The Elimination Matrix: Some Lemmas and Applications,”  SIAM. J. on Algebraic and Discrete Methods, vol. 1, no. 4, pp. 422–449, Dec. 1980, doi: 10.1137/0601049.\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.elimat-Tuple{Any}","page":"API Reference","title":"LiftAndLearn.elimat","text":"elimat(m::Integer) → L\n\nCreate elimination matrix L of dimension m [magnus1980].\n\nArguments\n\nm: dimension of the target matrix\n\nReturn\n\nL: elimination matrix\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.extractF-Tuple{Any, Any}","page":"API Reference","title":"LiftAndLearn.extractF","text":"extractF(F::Union{SparseMatrixCSC,VecOrMat}, r::Int) → F\n\nExtracting the F matrix for POD basis of dimensions (N, r)\n\nArguments\n\nF: F matrix\nr: reduced order\n\nReturns\n\nextracted F matrix\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.extractH-Tuple{Any, Any}","page":"API Reference","title":"LiftAndLearn.extractH","text":"extractH(H::Union{SparseMatrixCSC,VecOrMat}, r::Int) → H\n\nExtracting the H matrix for POD basis of dimensions (N, r)\n\nArguments\n\nH: H matrix\nr: reduced order\n\nReturns\n\nextracted H matrix\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.fidx-Tuple{Any, Any, Any}","page":"API Reference","title":"LiftAndLearn.fidx","text":"fidx(n::Int, j::Int, k::Int) → Int\n\nAuxiliary function for the F matrix indexing.\n\nArguments\n\nn: row dimension of the F matrix\nj: row index \nk: col index\n\nReturns\n\nindex corresponding to the F matrix\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.forwardEuler-Tuple{Function, Function, VecOrMat{T} where T, VecOrMat{T} where T}","page":"API Reference","title":"LiftAndLearn.forwardEuler","text":"forwardEuler(f, g, tdata, IC)\n\n\nForward euler scheme [dispatch for f(x,u) and u = g(u,t)]\n\nArguments\n\nf: Xdot = f(x,g(u,t)) right-hand-side of the dynamics\ng: Xdot = f(x,g(u,t)) input function g(u,t)\ntdata: time data\nIC: initial conditions\n\nReturn\n\nstates: integrated states\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.forwardEuler-Tuple{Function, Matrix{T} where T, VecOrMat{T} where T, VecOrMat{T} where T}","page":"API Reference","title":"LiftAndLearn.forwardEuler","text":"forwardEuler(f, U, tdata, IC)\n\n\nForward euler scheme [dispatch for f(x,u) and u-input as U-matrix]\n\nArguments\n\nf: Xdot = f(x,U) right-hand-side of the dynamics\nU: Xdot = f(x,U) input data U\ntdata: time data\nIC: initial conditions\n\nReturn\n\nstates: integrated states\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.forwardEuler-Tuple{Matrix{T} where T, Matrix{T} where T, Matrix{T} where T, VecOrMat{T} where T, VecOrMat{T} where T}","page":"API Reference","title":"LiftAndLearn.forwardEuler","text":"forwardEuler(A, B, U, tdata, IC)\n\n\nForward euler scheme integration.\n\nArguments\n\nA: linear state operator\nB: linear input operator\nU: input vector/matrix\ntdata: time data\nIC: initial conditions\n\nReturn\n\nstates: integrated states\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.getDataMat-Tuple{Matrix{T} where T, Union{Matrix{T} where T, LinearAlgebra.Transpose}, Matrix{T} where T, Dict, LiftAndLearn.Abstract_Options}","page":"API Reference","title":"LiftAndLearn.getDataMat","text":"getDataMat(Xhat::Matrix, Xhat_t::Union{Matrix,Transpose}, U::Matrix,\n    dims::Dict, options::Abstract_Options) → D\n\nGet the data matrix for the regression problem\n\nArguments\n\nXhat::Matrix: projected data matrix\nXhat_t::Union{Matrix,Transpose}: projected data matrix (transposed)\nU::Matrix: input data matrix\ndims::Dict: dictionary including important dimensions\noptions::Abstract_Options: options for the operator inference set by the user\n\nReturns\n\nD: data matrix for the regression problem\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.inferOp","page":"API Reference","title":"LiftAndLearn.inferOp","text":"inferOp(X::Matrix, U::Matrix, Y::VecOrMat, Vn::Matrix, R::Matrix,\n    options::Abstract_Options, IG::operators=operators()) → op::operators\n\nInfer the operators with derivative data given\n\nArguments\n\nX::Matrix: state data matrix\nU::Matrix: input data matrix\nY::VecOrMat: output data matix\nVn::Matrix: POD basis\nR::Matrix: derivative data matrix\noptions::Abstract_Options: options for the operator inference defined by the user\nIG::operators: initial guesses for optimization\n\nReturns\n\nop::operators: inferred operators\n\n\n\n\n\n","category":"function"},{"location":"api/#LiftAndLearn.inferOp-2","page":"API Reference","title":"LiftAndLearn.inferOp","text":"inferOp(W::Matrix, U::Matrix, Y::VecOrMat, Vn::Union{Matrix,BlockDiagonal},\n    lm::lifting, full_op::operators, options::Abstract_Options, \n    IG::operators=operators()) → op::operators\n\nInfer the operators for Lift And Learn for reprojected data (dispatch)\n\nArguments\n\nW::Matrix: state data matrix\nU::Matrix: input data matrix\nY::VecOrMat: output data matix\nVn::Union{Matrix,BlockDiagonal}: POD basis\nlm::lifting: struct of the lift map\nfull_op::operators: full order model operators\noptions::Abstract_Options: options for the operator inference defined by the user\nIG::operators: initial guesses for optimization\n\nReturns\n\nop::operators: inferred operators\n\n\n\n\n\n","category":"function"},{"location":"api/#LiftAndLearn.inferOp-3","page":"API Reference","title":"LiftAndLearn.inferOp","text":"inferOp(X::Matrix, U::Matrix, Y::VecOrMat, Vn::Matrix,\n    options::Abstract_Options, IG::operators=operators()) → op::operators\n\nInfer the operators without derivative data (dispatch)\n\nArguments\n\nX::Matrix: state data matrix\nU::Matrix: input data matrix\nY::VecOrMat: output data matix\nVn::Matrix: POD basis\noptions::Abstract_Options: options for the operator inference defined by the user\nIG::operators: initial guesses for optimization\n\nReturns\n\nop::operators: inferred operators\n\n\n\n\n\n","category":"function"},{"location":"api/#LiftAndLearn.inferOp-4","page":"API Reference","title":"LiftAndLearn.inferOp","text":"inferOp(X::Matrix, U::Matrix, Y::VecOrMat, Vn::Matrix,\n    full_op::operators, options::Abstract_Options, IG::operators=operators()) → op::operators\n\nInfer the operators with reprojection method (dispatch)\n\nArguments\n\nX::Matrix: state data matrix\nU::Matrix: input data matrix\nY::VecOrMat: output data matix\nVn::Matrix: POD basis\nfull_op::operators: full order model operators\noptions::Abstract_Options: options for the operator inference defined by the user\nIG::operators: initial guesses for optimization\n\nReturns\n\nop::operators: inferred operators\n\n\n\n\n\n","category":"function"},{"location":"api/#LiftAndLearn.insert2F-Tuple{Any, Any}","page":"API Reference","title":"LiftAndLearn.insert2F","text":"insertF(Fi::Union{SparseMatrixCSC,VecOrMat}, N::Int) → F\n\nInserting values into the F matrix for higher dimensions\n\nArguments\n\nFi: F matrix to insert\nN: the larger order\n\nReturns\n\ninserted F matrix\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.insert2H-Tuple{Any, Any}","page":"API Reference","title":"LiftAndLearn.insert2H","text":"insertH(Hi::Union{SparseMatrixCSC,VecOrMat}, N::Int) → H\n\nInserting values into the H matrix for higher dimensions\n\nArguments\n\nHi: H matrix to insert\nN: the larger order\n\nReturns\n\ninserted H matrix\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.insert2bilin-Tuple{Any, Any, Any}","page":"API Reference","title":"LiftAndLearn.insert2bilin","text":"insert2bilin(X::Union{SparseMatrixCSC,VecOrMat}, N::Int, p::Int) → BL\n\nInserting the values into the bilinear matrix (N) for higher dimensions\n\nArguments\n\nX: bilinear matrix to insert\nN: the larger order\n\nReturns\n\nInserted bilinear matrix\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.insert2randF-Tuple{Any, Any}","page":"API Reference","title":"LiftAndLearn.insert2randF","text":"insert2randF(Fi::Union{SparseMatrixCSC,VecOrMat}, N::Int) → F\n\nInserting values into the F matrix for higher dimensions\n\nArguments\n\nFi: F matrix to insert\nN: the larger order\n\nReturns\n\ninserted F matrix\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.intrusiveMR-Tuple{operators, AbstractArray, LiftAndLearn.Abstract_Options}","page":"API Reference","title":"LiftAndLearn.intrusiveMR","text":"intrusiveMR(op, Vr, options)\n\n\nPerform intrusive model reduction using Proper Orthogonal Decomposition (POD)\n\nArguments\n\nop: operators of the target system (A, B, C, F/H, N, K)\nVr: POD basis\noptions: options for the operator inference\n\nReturn\n\nop_new: new operator projected onto the basis\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.invec-Tuple{AbstractArray, Int64, Int64}","page":"API Reference","title":"LiftAndLearn.invec","text":"invec(r::AbstractArray, m::Int, n::Int) → r\n\nInverse vectorization.\n\nArguments\n\nr::AbstractArray: the input vector\nm::Int: the row dimension\nn::Int: the column dimension\n\nReturns\n\nthe inverse vectorized matrix\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.kronMatStates-Tuple{Any}","page":"API Reference","title":"LiftAndLearn.kronMatStates","text":"kronMatStates(Xmat::Union{SparseMatrixCSC,VecOrMat}) → Xkron\n\nGenerate the kronecker product state values (corresponding to the H matrix) for  a matrix form state data\n\nArguments\n\nXmat: state snapshot matrix\n\nReturns\n\nkronecker product state snapshot matrix\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.liftedBasis-Tuple{Matrix{T} where T, Real, Integer, Vector{T} where T}","page":"API Reference","title":"LiftAndLearn.liftedBasis","text":"liftedBasis(W, Nl, gp, ro) → Vr\n\nCreate the block-diagonal POD basis for the new lifted system data\n\nArguments\n\nw: lifted data matrix\nNl: number of variables of the lifted state dynamics\ngp: number of grid points for each variable\nro: vector of the reduced orders for each basis\n\nReturn\n\nVr: block diagonal POD basis\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.nommat-Tuple{Integer, Integer}","page":"API Reference","title":"LiftAndLearn.nommat","text":"nommat(m::Integer, n::Integer) → N\n\nCreate symmetric commutation matrix N of dimension m x n [magnus1980].\n\nArguments\n\nm::Integer: row dimension of the commutation matrix\nn::Integer: column dimension of the commutation matrix\n\nReturns\n\nN: symmetric commutation matrix\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.nommat-Tuple{Integer}","page":"API Reference","title":"LiftAndLearn.nommat","text":"nommat(m::Integer) → N\n\nDispatch for the symmetric commutation matrix of dimensions (m, m)\n\nArguments\n\nm::Integer: row and column dimension of the commutation matrix\n\nReturns\n\nN: symmetric commutation matrix\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.reproject-Tuple{Matrix{T} where T, Union{BlockDiagonals.BlockDiagonal, VecOrMat{T} where T}, VecOrMat{T} where T, Dict, lifting, operators, LiftAndLearn.Abstract_Options}","page":"API Reference","title":"LiftAndLearn.reproject","text":"inferOp(W::Matrix, U::Matrix, Y::VecOrMat, Vn::Union{Matrix,BlockDiagonal},\n    lm::lifting, options::Abstract_Options, IG::operators=operators()) → op::operators\n\nReprojecting the lifted data\n\nArguments\n\nXhat::Matrix: state data matrix projected onto the basis\nV::Union{VecOrMat,BlockDiagonal}: POD basis\nU::VecOrMat: input data matrix\ndims::Dict: dictionary including important dimensions\nlm::lifting: struct of the lift map\nop::operators: full order model operators\noptions::Abstract_Options: options for the operator inference defined by the user\n\nReturns\n\nRhat::Matrix: R matrix (transposed) for the regression problem\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.reproject-Tuple{Matrix{T} where T, Union{BlockDiagonals.BlockDiagonal, VecOrMat{T} where T}, VecOrMat{T} where T, Dict, operators, LiftAndLearn.Abstract_Options}","page":"API Reference","title":"LiftAndLearn.reproject","text":"inferOp(W::Matrix, U::Matrix, Y::VecOrMat, Vn::Union{Matrix,BlockDiagonal},\n    lm::lifting, options::Abstract_Options, IG::operators=operators()) → op::operators\n\nReprojecting the data to minimize the error affected by the missing orders of the POD basis\n\nArguments\n\nXhat::Matrix: state data matrix projected onto the basis\nV::Union{VecOrMat,BlockDiagonal}: POD basis\nU::VecOrMat: input data matrix\ndims::Dict: dictionary including important dimensions\nop::operators: full order model operators\noptions::Abstract_Options: options for the operator inference defined by the user\n\nReturn\n\nRhat::Matrix: R matrix (transposed) for the regression problem\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.run_optimizer","page":"API Reference","title":"LiftAndLearn.run_optimizer","text":"run_optimizer(D::AbstractArray, Rt::AbstractArray, Y::AbstractArray,\n    Xhat_t::AbstractArray, dims::Dict, options::Abstract_Options,\n    IG::operators=operators()) → op::operators\n\nRun the optimizer of choice.\n\nArguments\n\nD::AbstractArray: data matrix\nRt::AbstractArray: derivative data matrix (transposed)\nY::AbstractArray: output data matrix\nXhat_t::AbstractArray: projected data matrix (transposed)\ndims::Dict: dictionary including important dimensions\noptions::Abstract_Options: options for the operator inference set by the user\nIG::operators: initial guesses for optimization\n\nReturns\n\nop::operators: All learned operators \n\n\n\n\n\n","category":"function"},{"location":"api/#LiftAndLearn.semiImplicitEuler-NTuple{6, Any}","page":"API Reference","title":"LiftAndLearn.semiImplicitEuler","text":"semiImplicitEuler(A, B, F, U, tdata, IC)\n\n\nSemi-Implicit Euler scheme\n\nArguments\n\nA: linear state operator\nB: linear input operator\nF: quadratic state operator\nU: input data\ntdata: time data\nIC: initial condtions\n\nReturn\n\nstates: integrated states\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.semiImplicitEuler-NTuple{7, Any}","page":"API Reference","title":"LiftAndLearn.semiImplicitEuler","text":"semiImplicitEuler(A, B, F_or_H, U, tdata, IC, options)\n\n\nSemi-Implicit Euler scheme (dispatch)\n\nArguments\n\nA: linear state operator\nB: linear input operator\nF_or_H: quadratic state operator (F or H)\nU: input data\ntdata: time data\nIC: initial condtions\n\nReturn\n\nstates: integrated states\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.squareMatStates-Tuple{Any}","page":"API Reference","title":"LiftAndLearn.squareMatStates","text":"squareMatStates(Xmat::Union{SparseMatrixCSC,VecOrMat}) → Xsq\n\nGenerate the x^[2] squared state values (corresponding to the F matrix) for a  snapshot data matrix\n\nArguments\n\nXmat: state snapshot matrix\n\nReturns\n\nsquared state snapshot matrix \n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.tikhonov-Tuple{AbstractArray, AbstractArray, AbstractMatrix{T} where T, Real}","page":"API Reference","title":"LiftAndLearn.tikhonov","text":"tikhonov(b::AbstractArray, A::AbstractArray, Γ::AbstractMatrix, tol::Real;\n    flag::Bool=false) → x\n\nTikhonov regression\n\nArguments\n\nb::AbstractArray: right hand side of the regression problem\nA::AbstractArray: left hand side of the regression problem\nΓ::AbstractMatrix: Tikhonov matrix\ntol::Real: tolerance for the singular values\nflag::Bool: flag for the tolerance\n\nReturns\n\nregression solution\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.tikhonovMatrix!-Tuple{AbstractArray, Dict, LiftAndLearn.Abstract_Options}","page":"API Reference","title":"LiftAndLearn.tikhonovMatrix!","text":"tikhonovMatrix!(Γ::AbstractArray, dims::Dict, options::Abstract_Options)\n\nConstruct the Tikhonov matrix\n\nArguments\n\nΓ::AbstractArray: Tikhonov matrix (pass by reference)\ndims::Dict: dictionary including important dimensions\noptions::Abstract_Options: options for the operator inference set by the user\n\nReturns\n\nΓ: Tikhonov matrix (pass by reference)\n\n\n\n\n\n","category":"method"},{"location":"api/#LiftAndLearn.vech-Union{Tuple{AbstractMatrix{T}}, Tuple{T}} where T","page":"API Reference","title":"LiftAndLearn.vech","text":"vech(A::AbstractMatrix{T}) → v\n\nHalf-vectorization operation. For example half-vectorzation of\n\nA = beginbmatrix\n    a_11  a_12  \n    a_21  a_22\nendbmatrix\n\nbecomes\n\nv = beginbmatrix\n    a_11 \n    a_21 \n    a_22\nendbmatrix\n\nArguments\n\nA: matrix to half-vectorize\n\nReturns\n\nv: half-vectorized form\n\n\n\n\n\n","category":"method"},{"location":"models/FHN/#Fitzhugh-Nagumo-Equation","page":"FHN","title":"Fitzhugh-Nagumo Equation","text":"","category":"section"},{"location":"models/FHN/","page":"FHN","title":"FHN","text":"Modules = [LiftAndLearn.FHN]","category":"page"},{"location":"models/FHN/#LiftAndLearn.FHN","page":"FHN","title":"LiftAndLearn.FHN","text":"Fitzhugh-Nagumo PDE model\n\n\n\n\n\n","category":"module"},{"location":"models/FHN/#LiftAndLearn.FHN.Abstract_Models","page":"FHN","title":"LiftAndLearn.FHN.Abstract_Models","text":"Abstract_Models\n\nAbstract type for the models.\n\n\n\n\n\n","category":"type"},{"location":"models/FHN/#LiftAndLearn.FHN.fhn","page":"FHN","title":"LiftAndLearn.FHN.fhn","text":"mutable struct fhn <: LiftAndLearn.FHN.Abstract_Models\n\nFitzhugh-Nagumo PDE model\n\nbeginaligned\nfracpartial upartial t =  epsilon^2fracpartial^2 upartial x^2 + u(u-01)(1-u) - v + g \nfracpartial vpartial t = hu + gamma v + g\nendaligned\n\nwhere u and v are the state variables, g is the control input, and h, gamma, and epsilon are the parameters. Specifically, for this problem we assume the control input to begin\n\ng(t) = alpha t^3 exp(-beta t)\n\nwhere alpha and beta are the parameters that are going to be varied for training.\n\nFields\n\nΩ::Vector{Float64}: spatial domain\nT::Vector{Float64}: temporal domain\nαD::Vector{Float64}: alpha parameter domain\nβD::Vector{Float64}: beta parameter domain\nΔx::Float64: spatial grid size\nΔt::Float64: temporal step size\nUbc::Matrix{Float64}: boundary condition (input)\nICx::Matrix{Float64}: initial condition for the original states\nICw::Matrix{Float64}: initial condition for the lifted states\nx::Vector{Float64}: spatial grid points\nt::Vector{Float64}: temporal points\nXdim::Int64: spatial dimension\nTdim::Int64: temporal dimension\nFOM::Function: function to generate the full order operators\ngenerateFHNmatrices::Function: function to generate the full order operators for the intrusive model operators\n\n\n\n\n\n","category":"type"},{"location":"models/FHN/#LiftAndLearn.FHN.fhn-NTuple{6, Any}","page":"FHN","title":"LiftAndLearn.FHN.fhn","text":"fhn(Ω, T, αD, βD, Δx, Δt) → fhn\n\nFitzhugh-Nagumo PDE model\n\nArguments\n\nΩ::Vector{Float64}: spatial domain\nT::Vector{Float64}: temporal domain\nαD::Vector{Float64}: alpha parameter domain\nβD::Vector{Float64}: beta parameter domain\nΔx::Float64: spatial grid size\nΔt::Float64: temporal step size\n\nReturns\n\nfhn: Fitzhugh-Nagumo PDE model\n\n\n\n\n\n","category":"method"},{"location":"models/FHN/#LiftAndLearn.FHN.FOM-Tuple{Any, Any}","page":"FHN","title":"LiftAndLearn.FHN.FOM","text":"FOM(k, l) → A, B, C, K, f\n\nCreate the full order operators with the nonlinear operator expressed as f(x). \n\nArguments\n\nk::Int64: number of spatial grid points\nl::Float64: spatial domain length\n\nReturns\n\nA::SparseMatrixCSC{Float64,Int64}: A matrix\nB::SparseMatrixCSC{Float64,Int64}: B matrix\nC::SparseMatrixCSC{Float64,Int64}: C matrix\nK::SparseMatrixCSC{Float64,Int64}: K matrix\nf::Function: nonlinear operator\n\n\n\n\n\n","category":"method"},{"location":"models/FHN/#LiftAndLearn.FHN.generateFHNmatrices-Tuple{Any, Any}","page":"FHN","title":"LiftAndLearn.FHN.generateFHNmatrices","text":"generateFHNmatrices(k, l) → A, B, C, H, N, K\n\nGenerate the full order operators used for the intrusive model operators\n\nArguments\n\nk::Int64: number of spatial grid points\nl::Float64: spatial domain length\n\nReturns\n\nA::SparseMatrixCSC{Float64,Int64}: A matrix\nB::SparseMatrixCSC{Float64,Int64}: B matrix\nC::SparseMatrixCSC{Float64,Int64}: C matrix\nH::SparseMatrixCSC{Float64,Int64}: H matrix\nN::SparseMatrixCSC{Float64,Int64}: N matrix\nK::SparseMatrixCSC{Float64,Int64}: K matrix\n\n\n\n\n\n","category":"method"},{"location":"manual/Intrusive/#Intrusive-Model-Reduction","page":"Intrusive","title":"Intrusive Model Reduction","text":"","category":"section"},{"location":"manual/Intrusive/#POD","page":"Intrusive","title":"POD","text":"","category":"section"},{"location":"manual/Intrusive/","page":"Intrusive","title":"Intrusive","text":"Proper orthogonal decomposition originated from the analysis of turbulent flows in aerodynamics, and it has become one of the most widespread projection-based model reduction methods. POD reduces the model by projecting it onto a reduced subspace defined to be the span of basis vectors that optimally represent a set of simulation or experimental data. See the original literatures on POD [3], [4], [5].","category":"page"},{"location":"manual/Intrusive/","page":"Intrusive","title":"Intrusive","text":"In POD, we begin by collecting snapshots of state trajectory time series data by simulating the original full model ODE with K timesteps. We define the state snapshot data matrix as follows: ","category":"page"},{"location":"manual/Intrusive/","page":"Intrusive","title":"Intrusive","text":"    mathbfX = beginbmatrix \n               \n        boldsymbolmathbf x(t_1)  mathbfx(t_2)  cdots  mathbfx(t_K) \n               \n    endbmatrix in mathbbR^ntimes K ","category":"page"},{"location":"manual/Intrusive/","page":"Intrusive","title":"Intrusive","text":"More generally, the state snapshot matrix can contain state data from multiple simulations, e.g., from different initial conditions or using different parameters. Let mathbfX = mathbfVSigma W^top denote the singular value decomposition of the state snapshot. To reduce the dimension of the large-scale model, we denote by mathbfV_rin mathbb R^ntimes r the first r ll n columns of mathbf V; this is called the POD basis. Then, we approximate the state mathbfx in the subspace spanned by the POD basis, mathbf x approx mathbf V_r hatmathbf x where hatmathbf xinmathbbR^r is called the reduced state. If we substitute this approximation into a linear-quadratic system and enforce the Galerkin orthogonality condition that the approximation residual be orthogonal to the span of mathbf V_r, we arrive at a POD-Galerkin reduced model of the form","category":"page"},{"location":"manual/Intrusive/","page":"Intrusive","title":"Intrusive","text":"    dothatmathbf x(t) = mathbfhat Ahatmathbf x(t) + hatmathbfH(hatmathbfx(t) otimes hatmathbfx(t))","category":"page"},{"location":"manual/Intrusive/","page":"Intrusive","title":"Intrusive","text":"where the reduced operators are mathbfhatA = mathbfV^top_r mathbfAV_r in mathbbR^rtimes r and  hatmathbfH = mathbfV^top_r mathbfH(mathbfV_r otimes mathbfV_r) in mathbb R^rtimes r^2.","category":"page"},{"location":"manual/Intrusive/#Implementation","page":"Intrusive","title":"Implementation","text":"","category":"section"},{"location":"manual/Intrusive/","page":"Intrusive","title":"Intrusive","text":"The implementation of this corresponds to the following function:","category":"page"},{"location":"manual/Intrusive/","page":"Intrusive","title":"Intrusive","text":"intrusiveMR","category":"page"},{"location":"manual/Intrusive/#LiftAndLearn.intrusiveMR","page":"Intrusive","title":"LiftAndLearn.intrusiveMR","text":"intrusiveMR(op, Vr, options)\n\n\nPerform intrusive model reduction using Proper Orthogonal Decomposition (POD)\n\nArguments\n\nop: operators of the target system (A, B, C, F/H, N, K)\nVr: POD basis\noptions: options for the operator inference\n\nReturn\n\nop_new: new operator projected onto the basis\n\n\n\n\n\n","category":"function"},{"location":"manual/Intrusive/","page":"Intrusive","title":"Intrusive","text":"note: Note\nIn the next release, we will probably replace the function name intrusiveMR with pod for precision.","category":"page"},{"location":"manual/Utilities/#Utilities","page":"Utilities","title":"Utilities","text":"","category":"section"},{"location":"manual/Utilities/#System-operators","page":"Utilities","title":"System operators","text":"","category":"section"},{"location":"manual/Utilities/","page":"Utilities","title":"Utilities","text":"The struct operators provides a convenient way to organize and store the full and reduced operators.","category":"page"},{"location":"manual/Utilities/","page":"Utilities","title":"Utilities","text":"operators","category":"page"},{"location":"manual/Utilities/#LiftAndLearn.operators","page":"Utilities","title":"LiftAndLearn.operators","text":"mutable struct operators\n\nOrganize the operators of the system in a structure. The operators currently  supported are up to second order.\n\nFields\n\nA: linear state operator\nB: linear input operator\nC: linear output operator\nF: quadratic state operator with no redundancy\nH: quadratic state operator with redundancy\nK: constant operator\nN: bilinear (state-input) operator\nf: nonlinear function operator f(x,u)\n\n\n\n\n\n","category":"type"},{"location":"manual/Utilities/#Linear-Algebra","page":"Utilities","title":"Linear Algebra","text":"","category":"section"},{"location":"manual/Utilities/","page":"Utilities","title":"Utilities","text":"Many of the utility functions revolve around the properties of the following concepts","category":"page"},{"location":"manual/Utilities/","page":"Utilities","title":"Utilities","text":"Kronecker Products otimes\nVectorization textvec(cdot)\nHalf-vectorization textvech(cdot)","category":"page"},{"location":"manual/Utilities/","page":"Utilities","title":"Utilities","text":"The utility functions are based on the following key references ","category":"page"},{"location":"manual/Utilities/","page":"Utilities","title":"Utilities","text":"(1) Brewer, J. W. Kronecker Products and Matrix Calculus in System Theory IEEE Transactions on Circuits and Systems, 25(9) 772-781, 1978.","category":"page"},{"location":"manual/Utilities/","page":"Utilities","title":"Utilities","text":"@article{Brewer1978,\n  title = {Kronecker products and matrix calculus in system theory},\n  volume = {25},\n  ISSN = {0098-4094},\n  url = {http://dx.doi.org/10.1109/TCS.1978.1084534},\n  DOI = {10.1109/tcs.1978.1084534},\n  number = {9},\n  journal = {IEEE Transactions on Circuits and Systems},\n  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},\n  author = {Brewer,  J.},\n  year = {1978},\n  month = sep,\n  pages = {772–781}\n}","category":"page"},{"location":"manual/Utilities/","page":"Utilities","title":"Utilities","text":"(2) Magnus, J. R. and Neudecker, H. The Elimination Matrix: Some Lemmas and Applications. SIAM Journal on Algebraic Discrete Methods, 1(4) 422-449, 1980-12.","category":"page"},{"location":"manual/Utilities/","page":"Utilities","title":"Utilities","text":"@article{Magnus1980,\n  title = {The Elimination Matrix: Some Lemmas and Applications},\n  volume = {1},\n  ISSN = {2168-345X},\n  url = {http://dx.doi.org/10.1137/0601049},\n  DOI = {10.1137/0601049},\n  number = {4},\n  journal = {SIAM Journal on Algebraic Discrete Methods},\n  publisher = {Society for Industrial & Applied Mathematics (SIAM)},\n  author = {Magnus,  Jan R. and Neudecker,  H.},\n  year = {1980},\n  month = dec,\n  pages = {422–449}\n}","category":"page"},{"location":"manual/Utilities/#Utility-APIs","page":"Utilities","title":"Utility APIs","text":"","category":"section"},{"location":"manual/Utilities/","page":"Utilities","title":"Utilities","text":"vech\ninvec\ndupmat\nelimat\ncommat\nnommat\nF2H\nH2F\nF2Hs\nQ2H\nH2Q\nsquareMatStates\nkronMatStates\nextractF\ninsert2F\ninsert2randF\nextractH\ninsert2H\ninsert2bilin","category":"page"},{"location":"manual/Utilities/#LiftAndLearn.vech","page":"Utilities","title":"LiftAndLearn.vech","text":"vech(A::AbstractMatrix{T}) → v\n\nHalf-vectorization operation. For example half-vectorzation of\n\nA = beginbmatrix\n    a_11  a_12  \n    a_21  a_22\nendbmatrix\n\nbecomes\n\nv = beginbmatrix\n    a_11 \n    a_21 \n    a_22\nendbmatrix\n\nArguments\n\nA: matrix to half-vectorize\n\nReturns\n\nv: half-vectorized form\n\n\n\n\n\n","category":"function"},{"location":"manual/Utilities/#LiftAndLearn.invec","page":"Utilities","title":"LiftAndLearn.invec","text":"invec(r::AbstractArray, m::Int, n::Int) → r\n\nInverse vectorization.\n\nArguments\n\nr::AbstractArray: the input vector\nm::Int: the row dimension\nn::Int: the column dimension\n\nReturns\n\nthe inverse vectorized matrix\n\n\n\n\n\n","category":"function"},{"location":"manual/Utilities/#LiftAndLearn.dupmat","page":"Utilities","title":"LiftAndLearn.dupmat","text":"dupmat(n::Integer) → D\n\nCreate duplication matrix D of dimension n [magnus1980].\n\nArguments\n\nn: dimension of the duplication matrix\n\nReturns\n\nD: duplication matrix\n\nExamples\n\njulia> A = [1 2; 3 4]\n2×2 Matrix{Int64}:\n 1  3\n 3  4\n\njulia> D = LnL.dupmat(2)\n4×3 SparseArrays.SparseMatrixCSC{Float64, Int64} with 4 stored entries:\n 1.0   ⋅    ⋅ \n  ⋅   1.0   ⋅\n  ⋅   1.0   ⋅\n  ⋅    ⋅   1.0\n\njulia> D * LnL.vech(A)\n4-element Vector{Float64}:\n 1.0\n 3.0\n 3.0\n 4.0\n\njulia> a = vec(A)\n4-element Vector{Int64}:\n 1\n 3\n 3\n 4\n\nReferences\n\n[magnus1980] J. R. Magnus and H. Neudecker, “The Elimination Matrix: Some Lemmas and Applications,”  SIAM. J. on Algebraic and Discrete Methods, vol. 1, no. 4, pp. 422–449, Dec. 1980, doi: 10.1137/0601049.\n\n\n\n\n\n","category":"function"},{"location":"manual/Utilities/#LiftAndLearn.elimat","page":"Utilities","title":"LiftAndLearn.elimat","text":"elimat(m::Integer) → L\n\nCreate elimination matrix L of dimension m [magnus1980].\n\nArguments\n\nm: dimension of the target matrix\n\nReturn\n\nL: elimination matrix\n\n\n\n\n\n","category":"function"},{"location":"manual/Utilities/#LiftAndLearn.commat","page":"Utilities","title":"LiftAndLearn.commat","text":"commat(m::Integer, n::Integer) → K\n\nCreate commutation matrix K of dimension m x n [magnus1980].\n\nArguments\n\nm::Integer: row dimension of the commutation matrix\nn::Integer: column dimension of the commutation matrix\n\nReturns\n\nK: commutation matrix\n\n\n\n\n\ncommat(m::Integer) → K\n\nDispatch for the commutation matrix of dimensions (m, m)\n\nArguments\n\nm::Integer: row and column dimension of the commutation matrix\n\nReturns\n\nK: commutation matrix\n\n\n\n\n\n","category":"function"},{"location":"manual/Utilities/#LiftAndLearn.nommat","page":"Utilities","title":"LiftAndLearn.nommat","text":"nommat(m::Integer, n::Integer) → N\n\nCreate symmetric commutation matrix N of dimension m x n [magnus1980].\n\nArguments\n\nm::Integer: row dimension of the commutation matrix\nn::Integer: column dimension of the commutation matrix\n\nReturns\n\nN: symmetric commutation matrix\n\n\n\n\n\nnommat(m::Integer) → N\n\nDispatch for the symmetric commutation matrix of dimensions (m, m)\n\nArguments\n\nm::Integer: row and column dimension of the commutation matrix\n\nReturns\n\nN: symmetric commutation matrix\n\n\n\n\n\n","category":"function"},{"location":"manual/Utilities/#LiftAndLearn.F2H","page":"Utilities","title":"LiftAndLearn.F2H","text":"F2H(F::Union{SparseMatrixCSC,VecOrMat}) → H\n\nConvert the quadratic F operator into the H operator\n\nArguments\n\nF: F matrix\n\nReturns\n\nH: H matrix\n\n\n\n\n\n","category":"function"},{"location":"manual/Utilities/#LiftAndLearn.H2F","page":"Utilities","title":"LiftAndLearn.H2F","text":"H2F(H::Union{SparseMatrixCSC,VecOrMat}) → F\n\nConvert the quadratic H operator into the F operator\n\nArguments\n\nH: H matrix\n\nReturns\n\nF: F matrix\n\n\n\n\n\n","category":"function"},{"location":"manual/Utilities/#LiftAndLearn.F2Hs","page":"Utilities","title":"LiftAndLearn.F2Hs","text":"F2Hs(F::Union{SparseMatrixCSC,VecOrMat}) → Hs\n\nConvert the quadratic F operator into the symmetric H operator.\n\nThis guarantees that the H operator is symmetric. The difference from F2H is that we use the elimination matrix L and the symmetric commutation matrix N to multiply the F matrix.\n\nArguments\n\nF: F matrix\n\nReturns\n\nHs: symmetric H matrix\n\n\n\n\n\n","category":"function"},{"location":"manual/Utilities/#LiftAndLearn.Q2H","page":"Utilities","title":"LiftAndLearn.Q2H","text":"Q2H(Q::AbstractArray) → H\n\nConvert the quadratic Q operator into the H operator. The Q matrix is  a 3-dim tensor with dimensions (n x n x n). Thus,\n\nmathbfQ = beginbmatrix \n    mathbfQ_1  \n    mathbfQ_2  \n    vdots  \n    mathbfQ_n \nendbmatrix\nquad textwhere  mathbfQ_i in mathbbR^n times n\n\nArguments\n\nQ::AbstractArray: Quadratic matrix in the 3-dim tensor form with dimensions (n x n x n)\n\nReturns\n\nthe H quadratic matrix\n\n\n\n\n\n","category":"function"},{"location":"manual/Utilities/#LiftAndLearn.H2Q","page":"Utilities","title":"LiftAndLearn.H2Q","text":"H2Q(H::AbstractArray) → Q\n\nConvert the quadratic H operator into the Q operator\n\nArguments\n\nH::AbstractArray: Quadratic matrix of dimensions (n x n^2)\n\nReturns\n\nthe Q quadratic matrix of 3-dim tensor\n\n\n\n\n\n","category":"function"},{"location":"manual/Utilities/#LiftAndLearn.squareMatStates","page":"Utilities","title":"LiftAndLearn.squareMatStates","text":"squareMatStates(Xmat::Union{SparseMatrixCSC,VecOrMat}) → Xsq\n\nGenerate the x^[2] squared state values (corresponding to the F matrix) for a  snapshot data matrix\n\nArguments\n\nXmat: state snapshot matrix\n\nReturns\n\nsquared state snapshot matrix \n\n\n\n\n\n","category":"function"},{"location":"manual/Utilities/#LiftAndLearn.kronMatStates","page":"Utilities","title":"LiftAndLearn.kronMatStates","text":"kronMatStates(Xmat::Union{SparseMatrixCSC,VecOrMat}) → Xkron\n\nGenerate the kronecker product state values (corresponding to the H matrix) for  a matrix form state data\n\nArguments\n\nXmat: state snapshot matrix\n\nReturns\n\nkronecker product state snapshot matrix\n\n\n\n\n\n","category":"function"},{"location":"manual/Utilities/#LiftAndLearn.extractF","page":"Utilities","title":"LiftAndLearn.extractF","text":"extractF(F::Union{SparseMatrixCSC,VecOrMat}, r::Int) → F\n\nExtracting the F matrix for POD basis of dimensions (N, r)\n\nArguments\n\nF: F matrix\nr: reduced order\n\nReturns\n\nextracted F matrix\n\n\n\n\n\n","category":"function"},{"location":"manual/Utilities/#LiftAndLearn.insert2F","page":"Utilities","title":"LiftAndLearn.insert2F","text":"insertF(Fi::Union{SparseMatrixCSC,VecOrMat}, N::Int) → F\n\nInserting values into the F matrix for higher dimensions\n\nArguments\n\nFi: F matrix to insert\nN: the larger order\n\nReturns\n\ninserted F matrix\n\n\n\n\n\n","category":"function"},{"location":"manual/Utilities/#LiftAndLearn.insert2randF","page":"Utilities","title":"LiftAndLearn.insert2randF","text":"insert2randF(Fi::Union{SparseMatrixCSC,VecOrMat}, N::Int) → F\n\nInserting values into the F matrix for higher dimensions\n\nArguments\n\nFi: F matrix to insert\nN: the larger order\n\nReturns\n\ninserted F matrix\n\n\n\n\n\n","category":"function"},{"location":"manual/Utilities/#LiftAndLearn.extractH","page":"Utilities","title":"LiftAndLearn.extractH","text":"extractH(H::Union{SparseMatrixCSC,VecOrMat}, r::Int) → H\n\nExtracting the H matrix for POD basis of dimensions (N, r)\n\nArguments\n\nH: H matrix\nr: reduced order\n\nReturns\n\nextracted H matrix\n\n\n\n\n\n","category":"function"},{"location":"manual/Utilities/#LiftAndLearn.insert2H","page":"Utilities","title":"LiftAndLearn.insert2H","text":"insertH(Hi::Union{SparseMatrixCSC,VecOrMat}, N::Int) → H\n\nInserting values into the H matrix for higher dimensions\n\nArguments\n\nHi: H matrix to insert\nN: the larger order\n\nReturns\n\ninserted H matrix\n\n\n\n\n\n","category":"function"},{"location":"manual/Utilities/#LiftAndLearn.insert2bilin","page":"Utilities","title":"LiftAndLearn.insert2bilin","text":"insert2bilin(X::Union{SparseMatrixCSC,VecOrMat}, N::Int, p::Int) → BL\n\nInserting the values into the bilinear matrix (N) for higher dimensions\n\nArguments\n\nX: bilinear matrix to insert\nN: the larger order\n\nReturns\n\nInserted bilinear matrix\n\n\n\n\n\n","category":"function"},{"location":"models/Heat1D/#1D-Heat-Equation","page":"Heat1D","title":"1D Heat Equation","text":"","category":"section"},{"location":"models/Heat1D/","page":"Heat1D","title":"Heat1D","text":"Models are under an Abstract_Model","category":"page"},{"location":"models/Heat1D/","page":"Heat1D","title":"Heat1D","text":"LiftAndLearn.Abstract_Model","category":"page"},{"location":"models/Heat1D/#LiftAndLearn.Abstract_Model","page":"Heat1D","title":"LiftAndLearn.Abstract_Model","text":"Abstract_Model\n\nAbstract type for the model.\n\n\n\n\n\n","category":"type"},{"location":"models/Heat1D/","page":"Heat1D","title":"Heat1D","text":"Modules = [LiftAndLearn.Heat1D]","category":"page"},{"location":"models/Heat1D/#LiftAndLearn.Heat1D","page":"Heat1D","title":"LiftAndLearn.Heat1D","text":"1 Dimensional Heat Equation Model\n\n\n\n\n\n","category":"module"},{"location":"models/Heat1D/#LiftAndLearn.Heat1D.Abstract_Models","page":"Heat1D","title":"LiftAndLearn.Heat1D.Abstract_Models","text":"Abstract_Models\n\nAbstract type for the models.\n\n\n\n\n\n","category":"type"},{"location":"models/Heat1D/#LiftAndLearn.Heat1D.heat1d","page":"Heat1D","title":"LiftAndLearn.Heat1D.heat1d","text":"mutable struct heat1d <: LiftAndLearn.Heat1D.Abstract_Models\n\n1 Dimensional Heat Equation Model\n\nfracpartial upartial t = mufracpartial^2 upartial x^2\n\nFields\n\nOmega::Vector{Float64}: spatial domain\nT::Vector{Float64}: temporal domain\nD::Vector{Float64}: parameter domain\nΔx::Float64: spatial grid size\nΔt::Float64: temporal step size\nUbc::Matrix{Float64}: boundary condition (input)\nIC::Matrix{Float64}: initial condition\nx::Vector{Float64}: spatial grid points\nt::Vector{Float64}: temporal points\nμs::Vector{Float64}: parameter vector\nXdim::Int64: spatial dimension\nTdim::Int64: temporal dimension\nPdim::Int64: parameter dimension\ngenerateABmatrix::Function: function to generate A and B matrices\n\n\n\n\n\n","category":"type"},{"location":"models/Heat1D/#LiftAndLearn.Heat1D.heat1d-NTuple{6, Any}","page":"Heat1D","title":"LiftAndLearn.Heat1D.heat1d","text":"heat1d(Omega, T, D, Δx, Δt, Pdim)\n\n\n1 Dimensional Heat Equation Model\n\nArguments\n\nOmega::Vector{Float64}: spatial domain\nT::Vector{Float64}: temporal domain\nD::Vector{Float64}: parameter domain\nΔx::Float64: spatial grid size  \nΔt::Float64: temporal step size\nPdim::Int64: parameter dimension\n\nReturns\n\nheat1d: 1D heat equation model\n\n\n\n\n\n","category":"method"},{"location":"models/Heat1D/#LiftAndLearn.Heat1D.generateABmatrix-Tuple{Any, Any, Any}","page":"Heat1D","title":"LiftAndLearn.Heat1D.generateABmatrix","text":"generateABmatrix(N, μ, Δx)\n\n\nGenerate A and B matrices for the 1D heat equation.\n\nArguments\n\nN::Int64: number of spatial grid points\nμ::Float64: viscosity coefficients\nΔx::Float64: spatial grid size\n\nReturns\n\nA::Matrix{Float64}: A matrix\nB::Matrix{Float64}: B matrix\n\n\n\n\n\n","category":"method"},{"location":"#LiftAndLearn","page":"Home","title":"LiftAndLearn","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"LiftAndLearn.jl is an implementation of the Lift and Learn as well as the operator inference algorithm proposed in the papers listed in Key References.","category":"page"},{"location":"#Operator-Inference-(OpInf)","page":"Home","title":"Operator Inference (OpInf)","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Operator Inference is a scientific machine-learning framework used in data-driven modeling of dynamical systems that aims to learn the governing equations or operators from observed data without explicit knowledge of the underlying physics or dynamics (but with some information such as the structure, e.g., linear, quadratic, bilinear, etc.). To know more about OpInf, please refer to these resources by Willcox Research Group and ACE Lab. Or you can head over to the documentation page of this package about OpInf.","category":"page"},{"location":"#Lift-and-Learn-(LnL)","page":"Home","title":"Lift and Learn (LnL)","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Lift and Learn is a physics-informed method for learning low-dimensional models for large-scale dynamical systems. Lifting refers to the transformation of the original nonlinear system to a linear, quadratic, bilinear, or polynomial system by mapping the original state space to a new space with additional auxiliary variables. After lifting the system to a more approachable form, we can learn a reduced model using the OpInf approach. For more info, head over to the documentation on LnL.","category":"page"},{"location":"#Requirements","page":"Home","title":"Requirements","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"julia versions 1.8.5 >\nWe use Ipopt for the optimization (e.g., EP-OpInf)\nThis requires additional proprietary linear-solvers including ma86 and ma97. \nYou can run the code without it by changing the options. By default Ipopt will use MUMPS but we recommend you obtain and download HSL_jll.jl. You can find the instructions here.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To use LiftAndLearn, install Julia, then at the Julia REPL, type:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"LiftAndLearn\")\nusing LiftAndLearn","category":"page"},{"location":"#Features","page":"Home","title":"Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Features included in this package are the following:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Intrusive model reduction using Proper Orthogonal Decomposition (POD)\nNon-intrusive model reduction using the standard Operator Inference\nNon-intrusive model reduction for non-polynomial systems using Lift And Learn\nPhysics-informed Operator Inference approaches\nEnergy-preserving\nMore to come in the future ...","category":"page"},{"location":"","page":"Home","title":"Home","text":"note: Note\nWe are actively working to incorporate new features into this package.","category":"page"},{"location":"#Examples","page":"Home","title":"Examples","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you wish to give this package a try see our Jupyter Notebook examples, where you will find a variety of examples:","category":"page"},{"location":"","page":"Home","title":"Home","text":"1-dimensional heat equation\nViscous Burgers' equation\nFitzhugh-Nagumo equation\nKuramoto-Sivashinksy equation (chaotic system)","category":"page"},{"location":"","page":"Home","title":"Home","text":"If you prefer running scripts rather then notebooks, then see the example scripts.","category":"page"},{"location":"#Contributing","page":"Home","title":"Contributing","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you find any bugs or issues please follow the instructions:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Open an issue with clear explanation of bug. Recommended to have minimal reproduction example.\nIf you have patched the bug on your own, then create a pull request.\nFor further inquiries please contact tkoike3@gatech.edu.","category":"page"},{"location":"#License","page":"Home","title":"License","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The source code is distributed under MIT License.","category":"page"},{"location":"#Key-References","page":"Home","title":"Key References","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"(1) Peherstorfer, B. and Willcox, K.  Data-driven operator inference for non-intrusive projection-based model reduction. Computer Methods in Applied Mechanics and Engineering, 306:196-215, 2016. (Download)","category":"page"},{"location":"","page":"Home","title":"Home","text":"@article{Peherstorfer16DataDriven,\n    title   = {Data-driven operator inference for nonintrusive projection-based model reduction},\n    author  = {Peherstorfer, B. and Willcox, K.},\n    journal = {Computer Methods in Applied Mechanics and Engineering},\n    volume  = {306},\n    pages   = {196-215},\n    year    = {2016},\n}","category":"page"},{"location":"","page":"Home","title":"Home","text":"(2) Qian, E., Kramer, B., Marques, A., and Willcox, K.  Transform & Learn: A data-driven approach to nonlinear model reduction. In the AIAA Aviation 2019 Forum, June 17-21, Dallas, TX. (Download)","category":"page"},{"location":"","page":"Home","title":"Home","text":"@inbook{QKMW2019aviation,\n    author = {Qian, E. and Kramer, B. and Marques, A. N. and Willcox, K. E.},\n    title = {Transform \\&amp; Learn: A data-driven approach to nonlinear model reduction},\n    booktitle = {AIAA Aviation 2019 Forum},\n    doi = {10.2514/6.2019-3707},\n    URL = {https://arc.aiaa.org/doi/abs/10.2514/6.2019-3707},\n    eprint = {https://arc.aiaa.org/doi/pdf/10.2514/6.2019-3707}\n}","category":"page"},{"location":"","page":"Home","title":"Home","text":"(3) Qian, E., Kramer, B., Peherstorfer, B., and Willcox, K. Lift & Learn: Physics-informed machine learning for large-scale nonlinear dynamical systems, Physica D: Nonlinear Phenomena, 2020.","category":"page"},{"location":"","page":"Home","title":"Home","text":"@article{qian2020lift,\n    title={Lift \\& {L}earn: {P}hysics-informed machine learning for large-scale nonlinear dynamical systems},\n    author={Qian, E. and Kramer, B. and Peherstorfer, B. and Willcox, K.},\n    journal={Physica D: Nonlinear Phenomena},\n    volume={406},\n    pages={132401},\n    year={2020},\n    publisher={Elsevier}\n}","category":"page"},{"location":"","page":"Home","title":"Home","text":"(4) Qian, E., Farcas, I.-G., and Willcox, K. Reduced operator inference for nonlinear partial differential equations, SIAM Journal of Scientific Computing, 2022.","category":"page"},{"location":"","page":"Home","title":"Home","text":"@article{doi:10.1137/21M1393972,\n    author = {Qian, Elizabeth and Farca\\c{s}, Ionu\\c{t}-Gabriel and Willcox, Karen},\n    title = {Reduced Operator Inference for Nonlinear Partial Differential Equations},\n    journal = {SIAM Journal on Scientific Computing},\n    volume = {44},\n    number = {4},\n    pages = {A1934-A1959},\n    year = {2022},\n    doi = {10.1137/21M1393972},\n    URL = {https://doi.org/10.1137/21M1393972},\n    eprint = {https://doi.org/10.1137/21M1393972},\n}","category":"page"},{"location":"models/KS/#Kuramoto-Sivashinsky-Equation","page":"KS","title":"Kuramoto-Sivashinsky Equation","text":"","category":"section"},{"location":"models/KS/","page":"KS","title":"KS","text":"Modules = [LiftAndLearn.KS]","category":"page"},{"location":"models/KS/#LiftAndLearn.KS","page":"KS","title":"LiftAndLearn.KS","text":"Kuramoto-Sivashinsky equation PDE model\n\n\n\n\n\n","category":"module"},{"location":"models/KS/#LiftAndLearn.KS.Abstract_Models","page":"KS","title":"LiftAndLearn.KS.Abstract_Models","text":"Abstract_Models\n\nAbstract type for the models.\n\n\n\n\n\n","category":"type"},{"location":"models/KS/#LiftAndLearn.KS.ks","page":"KS","title":"LiftAndLearn.KS.ks","text":"mutable struct ks <: LiftAndLearn.KS.Abstract_Models\n\nKuramoto-Sivashinsky equation PDE model\n\nfracpartial upartial t = -mufracpartial^4 upartial x^4 - fracpartial^2 upartial x^2 - ufracpartial upartial x\n\nwhere u is the state variable and mu is the viscosity coefficient.\n\nFields\n\nOmega::Vector{Float64}: spatial domain\nT::Vector{Float64}: temporal domain\nD::Vector{Float64}: parameter domain\nnx::Float64: number of spatial grid points\nΔx::Float64: spatial grid size\nΔt::Float64: temporal step size\nIC::VecOrMat{Float64}: initial condition\nx::Vector{Float64}: spatial grid points\nt::Vector{Float64}: temporal points\nk::Vector{Float64}: Fourier modes\nμs::Union{Vector{Float64},Float64}: parameter vector\nXdim::Int64: spatial dimension\nTdim::Int64: temporal dimension\nPdim::Int64: parameter dimension\ntype::String: model type\nmodel_PS::Function: model using Pseudo-Spectral Method/Fast Fourier Transform\nmodel_PS_ew::Function: model using Pseudo-Spectral Method/Fast Fourier Transform (element-wise)\nmodel_SG::Function: model using Spectral-Galerkin Method\nmodel_FD::Function: model using Finite Difference\nintegrate_FD::Function: integrator using Crank-Nicholson Adams-Bashforth method\nintegrate_PS::Function: integrator using Crank-Nicholson Adams-Bashforth method in the Fourier space\nintegrate_PS_ew::Function: integrator using Crank-Nicholson Adams-Bashforth method in the Fourier space (element-wise)\nintegrate_SG::Function: integrator for second method of Fourier Transform without FFT\n\n\n\n\n\n","category":"type"},{"location":"models/KS/#LiftAndLearn.KS.ks-NTuple{7, Any}","page":"KS","title":"LiftAndLearn.KS.ks","text":"ks(Omega, T, D, nx, Δt, Pdim, type) → ks\n\nKuramoto-Sivashinsky equation PDE model constructor\n\nArguments\n\nOmega::Vector{Float64}: spatial domain\nT::Vector{Float64}: temporal domain\nD::Vector{Float64}: parameter domain\nnx::Float64: number of spatial grid points\nΔt::Float64: temporal step size\nPdim::Int64: parameter dimension\n\nReturns\n\nks: Kuramoto-Sivashinsky equation PDE model\n\n\n\n\n\n","category":"method"},{"location":"models/KS/#LiftAndLearn.KS.integrate_FD-NTuple{4, Any}","page":"KS","title":"LiftAndLearn.KS.integrate_FD","text":"integrate_FD(A, F, tdata, IC; const_stepsize=true, u2_lm1=nothing) → u\n\nIntegrator using Crank-Nicholson Adams-Bashforth method for (FD)\n\nArguments\n\nA: A matrix\nF: F matrix\ntdata: temporal points\nIC: initial condition\nconst_stepsize: whether to use a constant time step size\nu2_lm1: u2 at j-2\n\nReturns\n\nu: state matrix\n\n\n\n\n\n","category":"method"},{"location":"models/KS/#LiftAndLearn.KS.integrate_PS-NTuple{4, Any}","page":"KS","title":"LiftAndLearn.KS.integrate_PS","text":"integrate_PS(A, F, tdata, IC) → u, uhat\n\nIntegrator using Crank-Nicholson Adams-Bashforth method for (FFT)\n\nArguments\n\nA: A matrix\nF: F matrix\ntdata: temporal points\nIC: initial condition\n\nReturns\n\nu: state matrix\nuhat: state matrix in the Fourier space\n\n\n\n\n\n","category":"method"},{"location":"models/KS/#LiftAndLearn.KS.integrate_PS_ew-NTuple{4, Any}","page":"KS","title":"LiftAndLearn.KS.integrate_PS_ew","text":"integrate_PS_ew(A, F, tdata, IC) → u, uhat\n\nIntegrator using Crank-Nicholson Adams-Bashforth method for (FFT) (element-wise)\n\nArguments\n\nA: A matrix\nF: F matrix\ntdata: temporal points\nIC: initial condition\n\nReturns\n\nu: state matrix\nuhat: state matrix in the Fourier space\n\n\n\n\n\n","category":"method"},{"location":"models/KS/#LiftAndLearn.KS.integrate_SG-NTuple{4, Any}","page":"KS","title":"LiftAndLearn.KS.integrate_SG","text":"integrate_SG(A, F, tdata, IC) → u, uhat\n\nIntegrator for model produced with Spectral-Galerkin method.\n\nArguments\n\nA: A matrix\nF: F matrix\ntdata: temporal points\nIC: initial condition\n\nReturns\n\nu: state matrix\nuhat: state matrix in the Fourier space\n\n\n\n\n\n","category":"method"},{"location":"models/KS/#LiftAndLearn.KS.model_FD-Tuple{LiftAndLearn.KS.ks, Float64}","page":"KS","title":"LiftAndLearn.KS.model_FD","text":"model_FD(model, μ) → A, F\n\nGenerate A, F matrices for the Kuramoto-Sivashinsky equation using the Finite Difference method.\n\nArguments\n\nmodel: Kuramoto-Sivashinsky equation model\nμ: parameter value\n\nReturns\n\nA: A matrix\nF: F matrix\n\n\n\n\n\n","category":"method"},{"location":"models/KS/#LiftAndLearn.KS.model_PS-Tuple{LiftAndLearn.KS.ks, Float64}","page":"KS","title":"LiftAndLearn.KS.model_PS","text":"model_PS(model, μ) → A, F\n\nGenerate A, F matrices for the Kuramoto-Sivashinsky equation using the Pseudo-Spectral/Fast Fourier Transform method.\n\nArguments\n\nmodel: Kuramoto-Sivashinsky equation model\nμ: parameter value\n\nReturns\n\nA: A matrix\nF: F matrix  (take out 1.0im)\n\n\n\n\n\n","category":"method"},{"location":"models/KS/#LiftAndLearn.KS.model_PS_ew-Tuple{LiftAndLearn.KS.ks, Float64}","page":"KS","title":"LiftAndLearn.KS.model_PS_ew","text":"model_PS_ew(model, μ) → A, F\n\nGenerate A, F matrices for the Kuramoto-Sivashinsky equation using the Fast Fourier Transform method (element-wise).\n\nArguments\n\nmodel: Kuramoto-Sivashinsky equation model\nμ: parameter value\n\nReturns\n\nA: A matrix\nF: F matrix\n\n\n\n\n\n","category":"method"},{"location":"models/KS/#LiftAndLearn.KS.model_SG-Tuple{LiftAndLearn.KS.ks, Float64}","page":"KS","title":"LiftAndLearn.KS.model_SG","text":"model_SG(model, μ) → A, F\n\nGenerate A, F matrices for the Kuramoto-Sivashinsky equation using the Spectral-Galerkin method.\n\nArguments\n\nmodel: Kuramoto-Sivashinsky equation model\nμ: parameter value\n\nReturns\n\nA: A matrix\nF: F matrix\n\n\n\n\n\n","category":"method"},{"location":"models/KS/#LiftAndLearn.KS.vech-Union{Tuple{AbstractMatrix{T}}, Tuple{T}} where T","page":"KS","title":"LiftAndLearn.KS.vech","text":"vech(A) → v\n\nHalf-vectorization operation\n\nArguments\n\nA: matrix to half-vectorize\n\nReturns\n\nv: half-vectorized form\n\n\n\n\n\n","category":"method"}]
}
